"""
í•„í„°ë§ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
"""
import pytest
import time
import asyncio
from unittest.mock import AsyncMock, patch
from concurrent.futures import ThreadPoolExecutor

from app.services.content_filter import ContentFilter
from app.services.filter_service import FilterService


class TestFilterPerformance:
    """í•„í„°ë§ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def mock_filter_service(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© FilterService ëª¨í‚¹"""
        with patch("app.services.filter_service.ContentFilter") as mock_filter:
            service = FilterService()
            service.filter = mock_filter

            # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•œ mock ì„¤ì •
            mock_filter.process = AsyncMock(
                return_value={
                    "is_safe": True,
                    "safety_score": 0.8,
                    "filtered_content": "ì•ˆì „í•œ í…ŒìŠ¤íŠ¸ ì‘ë‹µì…ë‹ˆë‹¤.",
                    "filter_reason": "",
                    "risk_categories": [],
                    "retry_count": 0,
                }
            )

            return service

    @pytest.mark.asyncio
    async def test_filter_latency(self, mock_filter_service):
        """í•„í„°ë§ ì§€ì—°ì‹œê°„ í…ŒìŠ¤íŠ¸"""
        # Given: í…ŒìŠ¤íŠ¸ ì»¨í…ì¸ 
        test_content = "ê²½ì œ ì§€í‘œì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì„¤ëª…ì…ë‹ˆë‹¤."

        # When: í•„í„°ë§ ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        result = await mock_filter_service.filter_response(test_content)
        end_time = time.time()

        latency = end_time - start_time

        # Then: ì§€ì—°ì‹œê°„ ê²€ì¦
        assert latency < 0.1  # 100ms ì´í•˜ì—¬ì•¼ í•¨ (ëª¨í‚¹ëœ ìƒíƒœ)
        assert result["processing_time"] >= 0

        print(f"ğŸ“Š í•„í„°ë§ ì§€ì—°ì‹œê°„: {latency:.3f}ì´ˆ")

    @pytest.mark.asyncio
    async def test_concurrent_filtering(self, mock_filter_service):
        """ë™ì‹œ í•„í„°ë§ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # Given: ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì»¨í…ì¸ ë“¤
        test_contents = ["ê²½ì œ ì§€í‘œ ì„¤ëª… 1", "íˆ¬ì ì •ë³´ ì œê³µ 2", "ì‹œì¥ ë¶„ì„ ë‚´ìš© 3", "ê¸ˆìœµ ë°ì´í„° í•´ì„ 4", "ê²½ì œ ë™í–¥ ë¶„ì„ 5"]

        # When: ë™ì‹œ í•„í„°ë§ ì²˜ë¦¬
        start_time = time.time()

        tasks = [mock_filter_service.filter_response(content) for content in test_contents]
        results = await asyncio.gather(*tasks)

        end_time = time.time()
        total_time = end_time - start_time

        # Then: ì„±ëŠ¥ ê²€ì¦
        assert len(results) == len(test_contents)
        assert all(result["content"] for result in results)
        assert total_time < 0.5  # 500ms ì´í•˜ (ëª¨í‚¹ëœ ìƒíƒœ)

        avg_time_per_request = total_time / len(test_contents)
        print(f"ğŸ“Š ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥: {len(test_contents)}ê°œ ìš”ì²­, ì´ {total_time:.3f}ì´ˆ")
        print(f"ğŸ“Š ìš”ì²­ë‹¹ í‰ê·  ì‹œê°„: {avg_time_per_request:.3f}ì´ˆ")

    @pytest.mark.asyncio
    async def test_memory_usage_simulation(self, mock_filter_service):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸"""
        # Given: ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì»¨í…ì¸ 
        small_content = "ì§§ì€ ë‚´ìš©"
        medium_content = "ì¤‘ê°„ ê¸¸ì´ì˜ ì»¨í…ì¸  " * 50  # ~1KB
        large_content = "ê¸´ ì»¨í…ì¸  " * 1000  # ~10KB

        test_cases = [("small", small_content), ("medium", medium_content), ("large", large_content)]

        # When: ê° í¬ê¸°ë³„ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
        performance_results = {}

        for size_name, content in test_cases:
            start_time = time.time()
            result = await mock_filter_service.filter_response(content)
            processing_time = time.time() - start_time

            performance_results[size_name] = {
                "content_size": len(content),
                "processing_time": processing_time,
                "success": result["filtered"] is not None,
            }

        # Then: ê²°ê³¼ ê²€ì¦ ë° ì¶œë ¥
        for size_name, metrics in performance_results.items():
            assert metrics["success"] is True
            print(f"ğŸ“Š {size_name} ì»¨í…ì¸  ({metrics['content_size']}B): {metrics['processing_time']:.3f}ì´ˆ")

    @pytest.mark.asyncio
    async def test_error_recovery_performance(self, mock_filter_service):
        """ì˜¤ë¥˜ ë³µêµ¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # Given: ì‹¤íŒ¨ í›„ ì„±ê³µí•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤
        mock_filter_service.filter.process = AsyncMock(
            side_effect=[
                Exception("ì²« ë²ˆì§¸ ì‹¤íŒ¨"),
                Exception("ë‘ ë²ˆì§¸ ì‹¤íŒ¨"),
                {  # ì„¸ ë²ˆì§¸ì— ì„±ê³µ
                    "is_safe": True,
                    "safety_score": 0.8,
                    "filtered_content": "ë³µêµ¬ëœ ì‘ë‹µ",
                    "filter_reason": "",
                    "risk_categories": [],
                    "retry_count": 2,
                },
            ]
        )

        # When: ì˜¤ë¥˜ ë³µêµ¬ ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        result = await mock_filter_service.filter_response("í…ŒìŠ¤íŠ¸ ì»¨í…ì¸ ")
        recovery_time = time.time() - start_time

        # Then: ë³µêµ¬ ì„±ëŠ¥ ê²€ì¦
        assert "ì„œë¹„ìŠ¤ ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" in result["content"]  # ì˜¤ë¥˜ ì²˜ë¦¬ë¨
        assert recovery_time < 1.0  # 1ì´ˆ ì´ë‚´ ë³µêµ¬

        print(f"ğŸ“Š ì˜¤ë¥˜ ë³µêµ¬ ì‹œê°„: {recovery_time:.3f}ì´ˆ")

    @pytest.mark.asyncio
    async def test_scaling_performance(self, mock_filter_service):
        """í™•ì¥ì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # Given: ì ì§„ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ë¶€í•˜
        load_levels = [1, 5, 10, 20]
        performance_data = []

        for num_concurrent in load_levels:
            # When: ë™ì‹œ ìš”ì²­ ìˆ˜ë¥¼ ëŠ˜ë ¤ê°€ë©° í…ŒìŠ¤íŠ¸
            start_time = time.time()

            tasks = [mock_filter_service.filter_response(f"í…ŒìŠ¤íŠ¸ ì»¨í…ì¸  {i}") for i in range(num_concurrent)]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            total_time = time.time() - start_time

            # ì„±ê³µí•œ ìš”ì²­ ìˆ˜ ê³„ì‚°
            successful_results = [r for r in results if not isinstance(r, Exception)]
            success_rate = len(successful_results) / num_concurrent
            throughput = num_concurrent / total_time

            performance_data.append(
                {
                    "concurrent_requests": num_concurrent,
                    "total_time": total_time,
                    "success_rate": success_rate,
                    "throughput": throughput,
                }
            )

        # Then: í™•ì¥ì„± ê²€ì¦
        for data in performance_data:
            assert data["success_rate"] >= 0.9  # 90% ì´ìƒ ì„±ê³µë¥ 
            print(
                f"ğŸ“Š ë™ì‹œ ìš”ì²­ {data['concurrent_requests']}ê°œ: "
                f"{data['throughput']:.1f} req/sec, "
                f"ì„±ê³µë¥  {data['success_rate']:.1%}"
            )

        # ì²˜ë¦¬ëŸ‰ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§€ì§€ ì•ŠëŠ”ì§€ í™•ì¸
        first_throughput = performance_data[0]["throughput"]
        last_throughput = performance_data[-1]["throughput"]
        degradation_ratio = last_throughput / first_throughput

        assert degradation_ratio > 0.5  # ì²˜ë¦¬ëŸ‰ì´ 50% ì´í•˜ë¡œ ë–¨ì–´ì§€ì§€ ì•ŠìŒ
        print(f"ğŸ“Š í™•ì¥ì„± ì§€ìˆ˜: {degradation_ratio:.2f} (1.0ì´ ì´ìƒì )")


class TestRealWorldScenarios:
    """ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""

    @pytest.mark.asyncio
    async def test_typical_conversation_performance(self):
        """ì¼ë°˜ì ì¸ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # Given: ì‹¤ì œ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤
        conversation_turns = [
            "ì•ˆë…•í•˜ì„¸ìš”. ê²½ì œ ìƒí™©ì— ëŒ€í•´ ê¶ê¸ˆí•©ë‹ˆë‹¤.",
            "í˜„ì¬ ì¸í”Œë ˆì´ì…˜ ìƒí™©ì€ ì–´ë–¤ê°€ìš”?",
            "íˆ¬ìí•  ë•Œ ì£¼ì˜í•´ì•¼ í•  ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ê¸ˆë¦¬ ì¸ìƒì´ ì£¼ì‹ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?",
            "ê°ì‚¬í•©ë‹ˆë‹¤. ë§ì€ ë„ì›€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.",
        ]

        with patch("app.services.filter_service.ContentFilter") as mock_filter:
            filter_service = FilterService()
            filter_service.filter = mock_filter

            # ê° í„´ë³„ë¡œ ë‹¤ë¥¸ ì‘ë‹µ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
            mock_filter.process = AsyncMock(
                side_effect=[
                    {
                        "is_safe": True,
                        "safety_score": 0.9,
                        "filtered_content": f"ì•ˆì „í•œ ì‘ë‹µ {i}",
                        "filter_reason": "",
                        "risk_categories": [],
                        "retry_count": 0,
                    }
                    for i in range(len(conversation_turns))
                ]
            )

            # When: ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜
            conversation_start = time.time()
            results = []

            for turn in conversation_turns:
                turn_start = time.time()
                result = await filter_service.filter_response(turn)
                turn_time = time.time() - turn_start

                results.append({"turn": turn, "result": result, "turn_time": turn_time})

            total_conversation_time = time.time() - conversation_start

            # Then: ëŒ€í™” ì„±ëŠ¥ ê²€ì¦
            assert len(results) == len(conversation_turns)
            assert all(r["result"]["content"] for r in results)
            assert total_conversation_time < 5.0  # ì „ì²´ ëŒ€í™”ê°€ 5ì´ˆ ì´ë‚´

            avg_turn_time = total_conversation_time / len(conversation_turns)
            print(f"ğŸ“Š ëŒ€í™” ì´ ì‹œê°„: {total_conversation_time:.3f}ì´ˆ")
            print(f"ğŸ“Š í„´ë‹¹ í‰ê·  ì‹œê°„: {avg_turn_time:.3f}ì´ˆ")

    @pytest.mark.asyncio
    async def test_batch_content_processing(self):
        """ë°°ì¹˜ ì»¨í…ì¸  ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # Given: ë°°ì¹˜ ì²˜ë¦¬í•  ì»¨í…ì¸ ë“¤
        batch_contents = [
            "ê¸ˆìœµ ë‰´ìŠ¤ 1: ê¸ˆë¦¬ ë³€ë™ ì†Œì‹",
            "ê¸ˆìœµ ë‰´ìŠ¤ 2: ì£¼ì‹ ì‹œì¥ ë™í–¥",
            "ê¸ˆìœµ ë‰´ìŠ¤ 3: í™˜ìœ¨ ë³€í™” ë¶„ì„",
            "ê¸ˆìœµ ë‰´ìŠ¤ 4: ê²½ì œ ì§€í‘œ ë°œí‘œ",
            "ê¸ˆìœµ ë‰´ìŠ¤ 5: ê¸°ì—… ì‹¤ì  ë¶„ì„",
        ]

        with patch("app.services.filter_service.ContentFilter") as mock_filter:
            filter_service = FilterService()
            filter_service.filter = mock_filter

            mock_filter.process = AsyncMock(
                return_value={
                    "is_safe": True,
                    "safety_score": 0.85,
                    "filtered_content": "í•„í„°ë§ëœ ë‰´ìŠ¤ ë‚´ìš©",
                    "filter_reason": "",
                    "risk_categories": [],
                    "retry_count": 0,
                }
            )

            # When: ë°°ì¹˜ ì²˜ë¦¬
            batch_start = time.time()

            # ë°°ì¹˜ ì²˜ë¦¬ (ë™ì‹œ ì‹¤í–‰)
            batch_tasks = [filter_service.filter_response(content) for content in batch_contents]
            batch_results = await asyncio.gather(*batch_tasks)

            batch_time = time.time() - batch_start

            # Then: ë°°ì¹˜ ì„±ëŠ¥ ê²€ì¦
            assert len(batch_results) == len(batch_contents)
            assert all(result["content"] for result in batch_results)

            throughput = len(batch_contents) / batch_time
            print(f"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬: {len(batch_contents)}ê°œ í•­ëª©, {batch_time:.3f}ì´ˆ")
            print(f"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ëŸ‰: {throughput:.1f} items/sec")
