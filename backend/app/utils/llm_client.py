from __future__ import annotations

from typing import AsyncGenerator, List, Dict

from app.core.llm import LLMFactory, LangfuseManager
from langchain_core.prompts import ChatPromptTemplate


class LLMClient:
    """Simple abstraction over OpenAI and Anthropic chat APIs."""

    def __init__(self) -> None:
        # core ëª¨ë“ˆì˜ LLMFactory ì‚¬ìš© - ê³µí†µ ë¡œì§ ì¬ì‚¬ìš©
        self.llm = LLMFactory.create_llm()
        # Langfuse Manager ì´ˆê¸°í™”
        self.langfuse_manager = LangfuseManager(service_name="backend_chatbot")

    def _create_chain(self, messages: List[Dict[str, str]]):
        """Chain ìƒì„± ê³µí†µ ë¡œì§ (DRY ì›ì¹™ ì¤€ìˆ˜)"""
        # Langfuse input ì¶”ì ì„ ìœ„í•´ í…œí”Œë¦¿ ë³€ìˆ˜ ë°©ì‹ ì‚¬ìš©
        if len(messages) == 1:
            # ë‹¨ì¼ ë©”ì‹œì§€ì¸ ê²½ìš°
            prompt = ChatPromptTemplate.from_template("{user_input}")
        else:
            # ë‹¤ì¤‘ ë©”ì‹œì§€ì¸ ê²½ìš° (ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
            prompt = ChatPromptTemplate.from_template(
                "Previous conversation:\n{conversation_history}\n\nUser: {user_input}"
            )
        return prompt | self.llm

    def _prepare_input_data(self, messages: List[Dict[str, str]]) -> Dict[str, str]:
        """ë©”ì‹œì§€ë¥¼ Langfuse input í˜•íƒœë¡œ ë³€í™˜"""
        if not messages:
            return {"user_input": ""}
        
        # ë§ˆì§€ë§‰ user ë©”ì‹œì§€ ì°¾ê¸°
        user_input = ""
        for msg in reversed(messages):
            if msg["role"] == "user":
                user_input = msg["content"]
                break
        
        if len(messages) == 1:
            return {"user_input": user_input}
        else:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„± (ë§ˆì§€ë§‰ user ë©”ì‹œì§€ ì œì™¸)
            history_parts = []
            for msg in messages[:-1]:  # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸
                role = "User" if msg["role"] == "user" else "Assistant"
                history_parts.append(f"{role}: {msg['content']}")
            
            conversation_history = "\n".join(history_parts)
            return {
                "conversation_history": conversation_history,
                "user_input": user_input
            }

    async def stream_chat(self, messages: List[Dict[str, str]]) -> AsyncGenerator[str, None]:
        """Yield assistant message chunks using LCEL."""
        chain = self._create_chain(messages)
        
        # ë©”ì‹œì§€ ë¶„ì„ ë° ì…ë ¥ ë°ì´í„° êµ¬ì„±
        input_data = self._prepare_input_data(messages)
        
        # Langfuse callback ì„¤ì • (ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)
        config = self.langfuse_manager.get_callback_config()
        
        print(f"ğŸš€ Backend stream_chat ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€")
        print(f"ğŸ“ Input data: {input_data}")
        
        async for chunk in chain.astream(input_data, config=config):
            if chunk.content:
                yield chunk.content
        
        print(f"ğŸ“Š Backend stream_chat ì™„ë£Œ - Langfuse ì¶”ì ë¨")

    async def chat(self, messages: List[Dict[str, str]]) -> str:
        """Return full assistant message."""
        chain = self._create_chain(messages)
        
        # ë©”ì‹œì§€ ë¶„ì„ ë° ì…ë ¥ ë°ì´í„° êµ¬ì„±
        input_data = self._prepare_input_data(messages)
        
        # Langfuse callback ì„¤ì • (ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)
        config = self.langfuse_manager.get_callback_config()
        
        print(f"ğŸš€ Backend chat ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€")
        print(f"ğŸ“ Input data: {input_data}")
        
        result = await chain.ainvoke(input_data, config=config)
        
        print(f"ğŸ“Š Backend chat ì™„ë£Œ - Langfuse ì¶”ì ë¨")
        
        return result.content if hasattr(result, "content") else str(result)
