#!/usr/bin/env python3
"""LLM Client - OpenAI ë° Anthropic API ì¶”ìƒí™”"""

import logging
from typing import AsyncGenerator, Dict, List, Optional

from langchain_core.prompts import ChatPromptTemplate

from app.core.llm import LLMFactory, LangfuseManager
from app.core.langfuse_factory import LangfuseFactory

# Langfuse observe ë°ì½”ë ˆì´í„° ì„í¬íŠ¸
try:
    from langfuse import observe
    LANGFUSE_OBSERVE_AVAILABLE = True
except ImportError:
    LANGFUSE_OBSERVE_AVAILABLE = False
    observe = None

logger = logging.getLogger(__name__)


class LLMClient:
    """Simple abstraction over OpenAI and Anthropic chat APIs."""

    def __init__(self, user=None, langfuse_manager: Optional[LangfuseManager] = None) -> None:
        # core ëª¨ë“ˆì˜ LLMFactory ì‚¬ìš© - ê³µí†µ ë¡œì§ ì¬ì‚¬ìš©
        self.llm = LLMFactory.create_llm()
        # Langfuse Manager ì´ˆê¸°í™” (ì˜ì¡´ì„± ì£¼ì… ë˜ëŠ” ê¸°ë³¸ ìƒì„±)
        self.langfuse_manager = langfuse_manager or LangfuseFactory.create_app_manager(user)
        # user ì •ë³´ ì €ì¥
        self.user = user
        # í•„í„°ë§ ì„œë¹„ìŠ¤ ì§€ì—° ë¡œë“œ (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        self._filter_service = None

    @property
    def filter_service(self):
        """FilterService ì§€ì—° ë¡œë“œ"""
        if self._filter_service is None:
            from app.services.filter_service import FilterService

            self._filter_service = FilterService(user=self.user)
        return self._filter_service

    def _create_chain(self, messages: List[Dict[str, str]]):
        """Chain ìƒì„± ê³µí†µ ë¡œì§ (DRY ì›ì¹™ ì¤€ìˆ˜)"""
        # Langfuse input ì¶”ì ì„ ìœ„í•´ í…œí”Œë¦¿ ë³€ìˆ˜ ë°©ì‹ ì‚¬ìš©
        if len(messages) == 1:
            # ë‹¨ì¼ ë©”ì‹œì§€ì¸ ê²½ìš°
            prompt = ChatPromptTemplate.from_template("{user_input}")
        else:
            # ë‹¤ì¤‘ ë©”ì‹œì§€ì¸ ê²½ìš° (ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
            prompt = ChatPromptTemplate.from_template(
                "Previous conversation:\n{conversation_history}\n\nUser: {user_input}"
            )
        return prompt | self.llm

    def _prepare_input_data(self, messages: List[Dict[str, str]]) -> Dict[str, str]:
        """ë©”ì‹œì§€ë¥¼ Langfuse input í˜•íƒœë¡œ ë³€í™˜"""
        if not messages:
            return {"user_input": ""}

        # ë§ˆì§€ë§‰ user ë©”ì‹œì§€ ì°¾ê¸°
        user_input = ""
        for msg in reversed(messages):
            if msg["role"] == "user":
                user_input = msg["content"]
                break

        if len(messages) == 1:
            return {"user_input": user_input}
        else:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„± (ë§ˆì§€ë§‰ user ë©”ì‹œì§€ ì œì™¸)
            history_parts = []
            for msg in messages[:-1]:  # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸
                role = "User" if msg["role"] == "user" else "Assistant"
                history_parts.append(f"{role}: {msg['content']}")

            conversation_history = "\n".join(history_parts)
            return {"conversation_history": conversation_history, "user_input": user_input}

    @observe() if LANGFUSE_OBSERVE_AVAILABLE else lambda func: func
    async def stream_chat_observed(self, messages: List[Dict[str, str]]) -> AsyncGenerator[str, None]:
        """@observe() ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…"""
        if LANGFUSE_OBSERVE_AVAILABLE and self.langfuse_manager:
            # trace ë©”íƒ€ë°ì´í„° ì„¤ì •
            self.langfuse_manager.update_current_trace(
                name="stream_chat",
                input_data={"messages_count": len(messages), "service": "backend_chatbot"}
            )

        chain = self._create_chain(messages)
        input_data = self._prepare_input_data(messages)
        config = self.langfuse_manager.get_callback_config()

        print(f"ğŸš€ Backend stream_chat_observed ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€")
        print(f"ğŸ“ Input data: {input_data}")
        print(f"ğŸ‘¤ User ID: {self.langfuse_manager.user_id}, Session ID: {self.langfuse_manager.session_id}")

        async for chunk in chain.astream(input_data, config=config):
            if chunk.content:
                yield chunk.content

        if LANGFUSE_OBSERVE_AVAILABLE and self.langfuse_manager:
            self.langfuse_manager.update_current_trace(output_data={"status": "completed"})

        print(f"ğŸ“Š Backend stream_chat_observed ì™„ë£Œ - Langfuse ì¶”ì ë¨")

    async def stream_chat(self, messages: List[Dict[str, str]]) -> AsyncGenerator[str, None]:
        """Yield assistant message chunks using LCEL."""
        # @observe() ë°ì½”ë ˆì´í„°ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ìƒˆë¡œìš´ ë©”ì„œë“œ ì‚¬ìš©
        if LANGFUSE_OBSERVE_AVAILABLE:
            async for chunk in self.stream_chat_observed(messages):
                yield chunk
            return

        # ê¸°ì¡´ ë°©ì‹ (fallback)
        chain = self._create_chain(messages)
        input_data = self._prepare_input_data(messages)
        config = self.langfuse_manager.get_callback_config()

        print(f"ğŸš€ Backend stream_chat ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€")
        print(f"ğŸ“ Input data: {input_data}")
        print(f"ğŸ‘¤ User ID: {self.langfuse_manager.user_id}, Session ID: {self.langfuse_manager.session_id}")

        async for chunk in chain.astream(input_data, config=config):
            if chunk.content:
                yield chunk.content

        print(f"ğŸ“Š Backend stream_chat ì™„ë£Œ - Langfuse ì¶”ì ë¨")

    async def stream_chat_with_filter(
        self, messages: List[Dict[str, str]], safety_level: str = None, chunk_size: int = 50
    ) -> AsyncGenerator[str, None]:
        """
        í•„í„°ë§ì´ ì ìš©ëœ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…

        Args:
            messages: ëŒ€í™” ë©”ì‹œì§€ë“¤
            safety_level: ì•ˆì „ ìˆ˜ì¤€ (strict/moderate/permissive)
            chunk_size: ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ í¬ê¸°
        """
        try:
            print(f"ğŸ›¡ï¸ í•„í„°ë§ ì ìš©ëœ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€")

            # 1. ì „ì²´ ì‘ë‹µ ìƒì„±
            full_response = await self.chat(messages)
            print(f"ğŸ“ ì›ë³¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(full_response)}ê¸€ì")

            # 2. í•„í„°ë§ ì ìš©
            filter_result = await self.filter_service.filter_response(full_response, safety_level)

            filtered_content = filter_result["content"]

            # 3. í•„í„°ë§ ê²°ê³¼ ë¡œê¹…
            if filter_result["filtered"]:
                print(
                    f"âš ï¸ ì»¨í…ì¸  í•„í„°ë§ë¨: score={filter_result['safety_score']}, " f"reason={filter_result['filter_reason']}"
                )
            else:
                print(f"âœ… ì»¨í…ì¸  ì•ˆì „ í™•ì¸: score={filter_result['safety_score']}")

            # 4. í•„í„°ë§ëœ ì»¨í…ì¸ ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
            for i in range(0, len(filtered_content), chunk_size):
                chunk = filtered_content[i : i + chunk_size]
                yield chunk

            print(f"ğŸ¯ í•„í„°ë§ëœ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {len(filtered_content)}ê¸€ì ì „ì†¡")

        except Exception as e:
            print(f"âŒ í•„í„°ë§ëœ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒì‹œ ì•ˆì „í•œ ë©”ì‹œì§€ ë°˜í™˜
            error_message = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            yield error_message

    @observe() if LANGFUSE_OBSERVE_AVAILABLE else lambda func: func
    async def chat_observed(self, messages: List[Dict[str, str]]) -> str:
        """@observe() ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•œ ì±„íŒ…"""
        if LANGFUSE_OBSERVE_AVAILABLE and self.langfuse_manager:
            # trace ë©”íƒ€ë°ì´í„° ì„¤ì •
            self.langfuse_manager.update_current_trace(
                name="chat",
                input_data={"messages_count": len(messages), "service": "backend_chatbot"}
            )

        chain = self._create_chain(messages)
        input_data = self._prepare_input_data(messages)
        config = self.langfuse_manager.get_callback_config()

        print(f"ğŸš€ Backend chat_observed ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€")
        print(f"ğŸ“ Input data: {input_data}")
        print(f"ğŸ‘¤ User ID: {self.langfuse_manager.user_id}, Session ID: {self.langfuse_manager.session_id}")

        result = await chain.ainvoke(input_data, config=config)

        if LANGFUSE_OBSERVE_AVAILABLE and self.langfuse_manager:
            self.langfuse_manager.update_current_trace(output_data={"status": "completed"})

        print(f"ğŸ“Š Backend chat_observed ì™„ë£Œ - Langfuse ì¶”ì ë¨")

        return result.content if hasattr(result, "content") else str(result)

    async def chat(self, messages: List[Dict[str, str]]) -> str:
        """Return full assistant message."""
        # @observe() ë°ì½”ë ˆì´í„°ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ìƒˆë¡œìš´ ë©”ì„œë“œ ì‚¬ìš©
        if LANGFUSE_OBSERVE_AVAILABLE:
            return await self.chat_observed(messages)

        # ê¸°ì¡´ ë°©ì‹ (fallback)
        chain = self._create_chain(messages)
        input_data = self._prepare_input_data(messages)
        config = self.langfuse_manager.get_callback_config()

        print(f"ğŸš€ Backend chat ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€")
        print(f"ğŸ“ Input data: {input_data}")
        print(f"ğŸ‘¤ User ID: {self.langfuse_manager.user_id}, Session ID: {self.langfuse_manager.session_id}")

        result = await chain.ainvoke(input_data, config=config)

        print(f"ğŸ“Š Backend chat ì™„ë£Œ - Langfuse ì¶”ì ë¨")

        return result.content if hasattr(result, "content") else str(result)

    async def check_content_safety(self, content: str) -> Dict[str, any]:
        """
        ì»¨í…ì¸  ì•ˆì „ì„±ë§Œ ê²€ì‚¬ (í•„í„°ë§ ì—†ì´)

        Args:
            content: ê²€ì‚¬í•  ì»¨í…ì¸ 

        Returns:
            ì•ˆì „ì„± ë¶„ì„ ê²°ê³¼
        """
        return await self.filter_service.check_safety_only(content)
