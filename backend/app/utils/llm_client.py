from __future__ import annotations

from typing import AsyncGenerator, List, Dict

from app.core.llm import LLMFactory, LangfuseManager
from langchain_core.prompts import ChatPromptTemplate


class LLMClient:
    """Simple abstraction over OpenAI and Anthropic chat APIs."""

    def __init__(self) -> None:
        # core ëª¨ë“ˆì˜ LLMFactory ì‚¬ìš© - ê³µí†µ ë¡œì§ ì¬ì‚¬ìš©
        self.llm = LLMFactory.create_llm()
        # Langfuse Manager ì´ˆê¸°í™”
        self.langfuse_manager = LangfuseManager(service_name="backend_chatbot")
        # í•„í„°ë§ ì„œë¹„ìŠ¤ ì§€ì—° ë¡œë“œ (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        self._filter_service = None

    @property
    def filter_service(self):
        """FilterService ì§€ì—° ë¡œë“œ"""
        if self._filter_service is None:
            from app.services.filter_service import FilterService

            self._filter_service = FilterService()
        return self._filter_service

    def _create_chain(self, messages: List[Dict[str, str]]):
        """Chain ìƒì„± ê³µí†µ ë¡œì§ (DRY ì›ì¹™ ì¤€ìˆ˜)"""
        # Langfuse input ì¶”ì ì„ ìœ„í•´ í…œí”Œë¦¿ ë³€ìˆ˜ ë°©ì‹ ì‚¬ìš©
        if len(messages) == 1:
            # ë‹¨ì¼ ë©”ì‹œì§€ì¸ ê²½ìš°
            prompt = ChatPromptTemplate.from_template("{user_input}")
        else:
            # ë‹¤ì¤‘ ë©”ì‹œì§€ì¸ ê²½ìš° (ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
            prompt = ChatPromptTemplate.from_template(
                "Previous conversation:\n{conversation_history}\n\nUser: {user_input}"
            )
        return prompt | self.llm

    def _prepare_input_data(self, messages: List[Dict[str, str]]) -> Dict[str, str]:
        """ë©”ì‹œì§€ë¥¼ Langfuse input í˜•íƒœë¡œ ë³€í™˜"""
        if not messages:
            return {"user_input": ""}

        # ë§ˆì§€ë§‰ user ë©”ì‹œì§€ ì°¾ê¸°
        user_input = ""
        for msg in reversed(messages):
            if msg["role"] == "user":
                user_input = msg["content"]
                break

        if len(messages) == 1:
            return {"user_input": user_input}
        else:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„± (ë§ˆì§€ë§‰ user ë©”ì‹œì§€ ì œì™¸)
            history_parts = []
            for msg in messages[:-1]:  # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸
                role = "User" if msg["role"] == "user" else "Assistant"
                history_parts.append(f"{role}: {msg['content']}")

            conversation_history = "\n".join(history_parts)
            return {"conversation_history": conversation_history, "user_input": user_input}

    async def stream_chat(self, messages: List[Dict[str, str]]) -> AsyncGenerator[str, None]:
        """Yield assistant message chunks using LCEL."""
        chain = self._create_chain(messages)

        # ë©”ì‹œì§€ ë¶„ì„ ë° ì…ë ¥ ë°ì´í„° êµ¬ì„±
        input_data = self._prepare_input_data(messages)

        # Langfuse callback ì„¤ì • (ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)
        config = self.langfuse_manager.get_callback_config()

        print(f"ğŸš€ Backend stream_chat ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€")
        print(f"ğŸ“ Input data: {input_data}")

        async for chunk in chain.astream(input_data, config=config):
            if chunk.content:
                yield chunk.content

        print(f"ğŸ“Š Backend stream_chat ì™„ë£Œ - Langfuse ì¶”ì ë¨")

    async def stream_chat_with_filter(
        self, messages: List[Dict[str, str]], safety_level: str = None, chunk_size: int = 50
    ) -> AsyncGenerator[str, None]:
        """
        í•„í„°ë§ì´ ì ìš©ëœ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…

        Args:
            messages: ëŒ€í™” ë©”ì‹œì§€ë“¤
            safety_level: ì•ˆì „ ìˆ˜ì¤€ (strict/moderate/permissive)
            chunk_size: ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ í¬ê¸°
        """
        try:
            print(f"ğŸ›¡ï¸ í•„í„°ë§ ì ìš©ëœ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€")

            # 1. ì „ì²´ ì‘ë‹µ ìƒì„±
            full_response = await self.chat(messages)
            print(f"ğŸ“ ì›ë³¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(full_response)}ê¸€ì")

            # 2. í•„í„°ë§ ì ìš©
            filter_result = await self.filter_service.filter_response(full_response, safety_level)

            filtered_content = filter_result["content"]

            # 3. í•„í„°ë§ ê²°ê³¼ ë¡œê¹…
            if filter_result["filtered"]:
                print(
                    f"âš ï¸ ì»¨í…ì¸  í•„í„°ë§ë¨: score={filter_result['safety_score']}, " f"reason={filter_result['filter_reason']}"
                )
            else:
                print(f"âœ… ì»¨í…ì¸  ì•ˆì „ í™•ì¸: score={filter_result['safety_score']}")

            # 4. í•„í„°ë§ëœ ì»¨í…ì¸ ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
            for i in range(0, len(filtered_content), chunk_size):
                chunk = filtered_content[i : i + chunk_size]
                yield chunk

            print(f"ğŸ¯ í•„í„°ë§ëœ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {len(filtered_content)}ê¸€ì ì „ì†¡")

        except Exception as e:
            print(f"âŒ í•„í„°ë§ëœ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒì‹œ ì•ˆì „í•œ ë©”ì‹œì§€ ë°˜í™˜
            error_message = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            yield error_message

    async def chat(self, messages: List[Dict[str, str]]) -> str:
        """Return full assistant message."""
        chain = self._create_chain(messages)

        # ë©”ì‹œì§€ ë¶„ì„ ë° ì…ë ¥ ë°ì´í„° êµ¬ì„±
        input_data = self._prepare_input_data(messages)

        # Langfuse callback ì„¤ì • (ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)
        config = self.langfuse_manager.get_callback_config()

        print(f"ğŸš€ Backend chat ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€")
        print(f"ğŸ“ Input data: {input_data}")

        result = await chain.ainvoke(input_data, config=config)

        print(f"ğŸ“Š Backend chat ì™„ë£Œ - Langfuse ì¶”ì ë¨")

        return result.content if hasattr(result, "content") else str(result)

    async def check_content_safety(self, content: str) -> Dict[str, any]:
        """
        ì»¨í…ì¸  ì•ˆì „ì„±ë§Œ ê²€ì‚¬ (í•„í„°ë§ ì—†ì´)

        Args:
            content: ê²€ì‚¬í•  ì»¨í…ì¸ 

        Returns:
            ì•ˆì „ì„± ë¶„ì„ ê²°ê³¼
        """
        return await self.filter_service.check_safety_only(content)
