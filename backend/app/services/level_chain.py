import logging
from typing import TypedDict
from langgraph.graph import StateGraph, START, END

from app.utils.llm_client import LLMClient
from app.constants import UserLevel
from app.core.prompts import SYSTEM_PROMPTS, SEARCH_DECISION_PROMPT
from app.services.simple_search import search_web

# Langfuse observe ë°ì½”ë ˆì´í„° ì„í¬íŠ¸
try:
    from langfuse import observe

    LANGFUSE_OBSERVE_AVAILABLE = True
except ImportError:
    LANGFUSE_OBSERVE_AVAILABLE = False
    observe = None

logger = logging.getLogger(__name__)


class LevelChainState(TypedDict):
    """ë ˆë²¨ë³„ ì²´ì¸ ìƒíƒœ - ìˆœìˆ˜ ë ˆë²¨ë³„ ë¡œì§"""

    user_level: UserLevel
    user_query: str
    conversation_history: list[dict[str, str]]
    memory_context: str
    final_response: str

    # ë ˆë²¨ë³„ ì „ì²˜ë¦¬ ìƒíƒœ
    needs_search: bool
    search_results: str
    tools_used: list[str]
    processing_step: str
    error_message: str


class BaseLevelChain:
    """ë ˆë²¨ë³„ LLM ì²´ì¸ - LLMClient í™œìš©ìœ¼ë¡œ Langfuse ì¶”ì  í¬í•¨"""

    def __init__(self, user_level: UserLevel, user=None, langfuse_manager=None, recursion_limit=30):
        self.llm_client = LLMClient(user=user, langfuse_manager=langfuse_manager)
        self.user_level = user_level
        self.user = user
        self.langfuse_manager = langfuse_manager
        self.graph = self._build_graph()
        self.config = {"recursion_limit": recursion_limit}

    def _build_graph(self) -> StateGraph:
        """ê¸°ë³¸ ë”ë¯¸ ê·¸ë˜í”„ - ê° ë ˆë²¨ì—ì„œ ì˜¤ë²„ë¼ì´ë“œí•˜ì—¬ ì‚¬ìš©"""
        workflow = StateGraph(LevelChainState)

        # ğŸ¯ ê°„ë‹¨í•œ ë”ë¯¸ ë…¸ë“œ - ì‹¤ì œë¡œëŠ” ê° ë ˆë²¨ì—ì„œ êµ¬í˜„
        workflow.add_node("dummy_response", self._dummy_response)
        workflow.add_edge(START, "dummy_response")
        workflow.add_edge("dummy_response", END)

        return workflow.compile()

    @staticmethod
    async def _dummy_response(state: LevelChainState) -> LevelChainState:
        """ë”ë¯¸ ì‘ë‹µ - ì‹¤ì œë¡œëŠ” ê° ë ˆë²¨ì—ì„œ êµ¬í˜„ëœ ë©”ì„œë“œê°€ ì‚¬ìš©ë¨"""
        state["final_response"] = "ê° ë ˆë²¨ì—ì„œ êµ¬í˜„ëœ ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤."
        state["tools_used"].append("dummy")
        return state

    @staticmethod
    def _finalize_response(state: LevelChainState) -> LevelChainState:
        """ê³µí†µ ì‘ë‹µ ìµœì¢…í™” - ëª¨ë“  ë ˆë²¨ì—ì„œ ì¬ì‚¬ìš©"""
        logger.info(f"ì‘ë‹µ ìµœì¢…í™”: {state.get('user_level', 'unknown')}")
        state["processing_step"] = "ì™„ë£Œ"

        if not state.get("final_response"):
            state["final_response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        return state

    @staticmethod
    async def _web_search(state: LevelChainState) -> LevelChainState:
        """ì›¹ ê²€ìƒ‰"""
        try:
            logger.info("ì›¹ ê²€ìƒ‰ ì‹¤í–‰")
            state["processing_step"] = "web_search"

            search_results = await search_web(state["user_query"])
            state["search_results"] = search_results
            state["tools_used"].append("web_search")

            logger.info("ì›¹ ê²€ìƒ‰ ì™„ë£Œ")
            return state

        except Exception as e:
            logger.error(f"âŒ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            state["search_results"] = f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
            state["tools_used"].append("web_search_failed")
            return state

    @staticmethod
    def _route_analysis(state: LevelChainState) -> str:
        """ë¶„ì„ í›„ ë¼ìš°íŒ…"""
        if state.get("error_message"):
            return "response"  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì‘ë‹µ
        elif state["needs_search"]:
            return "search"
        else:
            return "response"

    @ observe() if LANGFUSE_OBSERVE_AVAILABLE else lambda func: func
    async def process(
        self,
        user_level: UserLevel,
        user_query: str,
        conversation_history: list[dict[str, str]] = None,
        memory_context: str = None,
    ) -> LevelChainState:
        """ë ˆë²¨ë³„ ì²´ì¸ ì²˜ë¦¬ - Langfuse ì¶”ì  í¬í•¨"""

        # Langfuse ë©”íƒ€ë°ì´í„° ì„¤ì • (LLMClient íŒ¨í„´ê³¼ ë™ì¼)
        if LANGFUSE_OBSERVE_AVAILABLE and self.langfuse_manager:
            self.langfuse_manager.update_current_trace(
                name="level_chain_process",
                input_data={
                    "user_level": user_level.value,
                    "query_length": len(user_query),
                    "has_memory": bool(memory_context),
                    "history_count": len(conversation_history or []),
                    "service": "level_chain",
                },
            )
        initial_state = LevelChainState(
            user_level=user_level,
            user_query=user_query,
            conversation_history=conversation_history or [],
            memory_context=memory_context or "",
            final_response="",
            needs_search=False,
            search_results="",
            tools_used=[],
            processing_step="ì‹œì‘",
            error_message="",
        )

        try:
            # ê·¸ë˜í”„ ì‹¤í–‰

            result = await self.graph.ainvoke(initial_state, config=self.config)

            # Langfuse ê²°ê³¼ ì—…ë°ì´íŠ¸ (LLMClient íŒ¨í„´ê³¼ ë™ì¼)
            if LANGFUSE_OBSERVE_AVAILABLE and self.langfuse_manager:
                self.langfuse_manager.update_current_trace(
                    output_data={
                        "status": "completed",
                        "tools_used": result.get("tools_used", []),
                        "response_length": len(result.get("final_response", "")),
                    }
                )

            logger.info(f"ë ˆë²¨ë³„ ì²´ì¸ ì™„ë£Œ: level={user_level.value}, tools={result['tools_used']} (Langfuse ì¶”ì ë¨)")
            return result

        except Exception as e:
            logger.error(f"ë ˆë²¨ë³„ ì²´ì¸ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì²˜ë¦¬
            return {
                **initial_state,
                "final_response": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "error_message": str(e),
                "processing_step": "ì˜¤ë¥˜",
            }

    def _get_system_prompt(self) -> str:
        # ë ˆë²¨ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
        return SYSTEM_PROMPTS.get(self.user_level, "")

    async def _generate_response(self, state: LevelChainState) -> LevelChainState:
        """LLM ì‘ë‹µ - ê¸°ì¡´ LLMClient ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ í™œìš©"""
        try:
            state["processing_step"] = "LLM ì‘ë‹µ ìƒì„±"

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            system_prompt = self._get_system_prompt()

            # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ì´ì „ ëŒ€í™” ê¸°ì–µ)
            if state["memory_context"]:
                system_prompt += f"\n\n[ì´ì „ ëŒ€í™” ê¸°ì–µ]\n{state['memory_context']}"

            # ğŸ” ì‹¤ì „ëŸ¬ì˜ ê²½ìš° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (AdvancedLevelChainì—ì„œë§Œ í•´ë‹¹)
            if hasattr(self, "user_level") and self.user_level == UserLevel.ADVANCED and state.get("search_results"):
                system_prompt += f"\n\n[ì‹¤ì‹œê°„ ì‹œì¥ ì •ë³´ ë° ìµœì‹  ë°ì´í„°]\n{state['search_results']}"
                state["tools_used"].append("llm_with_search")
            else:
                state["tools_used"].append("llm_basic")

            # ë©”ì‹œì§€ êµ¬ì„± (ê¸°ì¡´ LLMClient ë°©ì‹ê³¼ ë™ì¼)
            messages = [{"role": "system", "content": system_prompt}]
            messages.extend(state["conversation_history"])
            messages.append({"role": "user", "content": state["user_query"]})

            # ğŸ¯ LLMClient ì§ì ‘ í™œìš© - Langfuse ì¶”ì  ìë™ í¬í•¨!
            response = await self.llm_client.chat(messages)

            state["final_response"] = response
            return state

        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            state["error_message"] = f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}"
            state["final_response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return state

    async def _analyze_query(self, state: LevelChainState) -> LevelChainState:
        """AIë¥¼ í†µí•œ ì›¹ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨"""
        try:
            logger.info("AI ê¸°ë°˜ ì¿¼ë¦¬ ë¶„ì„")
            state["processing_step"] = "AI ì¿¼ë¦¬ ë¶„ì„"

            user_query = state["user_query"]

            # AIì—ê²Œ ê²€ìƒ‰ í•„ìš”ì„± ì§ˆë¬¸
            messages = [{"role": "user", "content": SEARCH_DECISION_PROMPT.format(user_query=user_query)}]
            # TODO ì €ë ´í•œ ëª¨ë¸ë¡œ êµì²´ ê³ ë ¤ í•„ìš”
            ai_decision = await self.llm_client.chat(messages)

            # AI ì‘ë‹µ ë¶„ì„ (YES/NO íŒë‹¨)
            ai_decision_clean = ai_decision.strip().upper()
            needs_search = "YES" in ai_decision_clean

            state["needs_search"] = needs_search
            state["tools_used"].append("ai_search_analysis")

            logger.info(f"AI ë¶„ì„ ì™„ë£Œ: needs_search={needs_search}, AI ì‘ë‹µ='{ai_decision_clean}'")
            return state

        except Exception as e:
            logger.error(f"AI ì¿¼ë¦¬ ë¶„ì„ ì˜¤ë¥˜: {e}")
            state["error_message"] = f"AI ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
            # ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ ê²€ìƒ‰í•˜ì§€ ì•ŠìŒìœ¼ë¡œ ì„¤ì •
            state["needs_search"] = False
            return state


class BeginnerLevelChain(BaseLevelChain):
    """ğŸ”° ì£¼ë¦°ì´ ì „ìš© ì²´ì¸ - ê¸°ì¡´ LLM ì²´ì¸ ì—­í•  ëŒ€ì²´"""

    def _build_graph(self) -> StateGraph:
        """ì£¼ë¦°ì´ìš© ìµœë‹¨ ì›Œí¬í”Œë¡œìš° - ë³µì¡í•œ ë¶„ì„ ì—†ì´ ë°”ë¡œ LLM ì‘ë‹µ"""
        workflow = StateGraph(LevelChainState)

        # ğŸ¯ ê°„ë‹¨í•œ ì§ì„  í”Œë¡œìš° - ë¶„ì„ì´ë‚˜ ê²€ìƒ‰ ì—†ì´ ë°”ë¡œ LLM í˜¸ì¶œ
        workflow.add_node("generate_response", self._generate_response)
        workflow.add_node("finalize", self._finalize_response)

        # ì§ì„  ì—°ê²°: START â†’ LLM ì‘ë‹µ â†’ ìµœì¢…í™” â†’ END
        workflow.add_edge(START, "generate_response")
        workflow.add_edge("generate_response", "finalize")
        workflow.add_edge("finalize", END)

        return workflow.compile()


class IntermediateLevelChain(BaseLevelChain):
    """ğŸ“ˆ ê´€ì‹¬ëŸ¬ ì „ìš© ì²´ì¸"""

    def _build_graph(self) -> StateGraph:
        # TODO ì£¼ë¦°ì´ ì „ìš© ì²´ì¸ê³¼ ì°¨ë³„í™” í•„ìš”
        workflow = StateGraph(LevelChainState)

        # ğŸ“ˆ ê°„ë‹¨í•œ ì§ì„  í”Œë¡œìš° - ê²€ìƒ‰ ì—†ì´ ê¸°ë³¸ LLM ì‘ë‹µë§Œ
        workflow.add_node("generate_response", self._generate_response)
        workflow.add_node("finalize", self._finalize_response)

        # ì§ì„  ì—°ê²°: START â†’ LLM ì‘ë‹µ â†’ ìµœì¢…í™” â†’ END
        workflow.add_edge(START, "generate_response")
        workflow.add_edge("generate_response", "finalize")
        workflow.add_edge("finalize", END)

        return workflow.compile()


class AdvancedLevelChain(BaseLevelChain):
    """ğŸ¯ ì‹¤ì „ëŸ¬ ì „ìš© ì²´ì¸ - ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ í¬í•¨"""

    def _build_graph(self) -> StateGraph:
        """ì‹¤ì „ëŸ¬ìš© ê³ ê¸‰ ì›Œí¬í”Œë¡œìš° - ê²€ìƒ‰ í‚¤ì›Œë“œ ê°ì§€ ì‹œ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
        workflow = StateGraph(LevelChainState)

        # ğŸ¯ ì‹¤ì „ëŸ¬ ì „ìš© ë…¸ë“œë“¤
        workflow.add_node("analyze_query", self._analyze_query)
        workflow.add_node("web_search", self._web_search)
        workflow.add_node("generate_response", self._generate_response)
        workflow.add_node("finalize", self._finalize_response)

        # í”Œë¡œìš° êµ¬ì„±: ë¶„ì„ â†’ ì¡°ê±´ë¶€ ê²€ìƒ‰ â†’ LLM ì‘ë‹µ
        workflow.add_edge(START, "analyze_query")

        # ì‹¤ì „ëŸ¬ ì „ìš© ë¼ìš°íŒ…
        workflow.add_conditional_edges(
            "analyze_query",
            self._route_analysis,
            {"search": "web_search", "response": "generate_response"},
        )

        # ê²€ìƒ‰ í›„ ê³ ê¸‰ ì‘ë‹µ ìƒì„±
        workflow.add_edge("web_search", "generate_response")
        workflow.add_edge("generate_response", "finalize")
        workflow.add_edge("finalize", END)

        return workflow.compile()


class LevelChainService:
    """ë ˆë²¨ë³„ LLM ì²´ì¸ ì„œë¹„ìŠ¤"""

    def __init__(self, user=None, langfuse_manager=None):
        self.user = user
        self.langfuse_manager = langfuse_manager

    def get_chain(self, user_level: UserLevel) -> BaseLevelChain:
        """ë ˆë²¨ì— ë§ëŠ” ì²´ì¸ ë°˜í™˜"""

        if user_level == UserLevel.BEGINNER:
            return BeginnerLevelChain(user_level=user_level, user=self.user, langfuse_manager=self.langfuse_manager)
        elif user_level == UserLevel.INTERMEDIATE:
            return IntermediateLevelChain(user_level=user_level, user=self.user, langfuse_manager=self.langfuse_manager)
        elif user_level == UserLevel.ADVANCED:
            return AdvancedLevelChain(user_level=user_level, user=self.user, langfuse_manager=self.langfuse_manager)
        else:
            raise NotImplementedError

    @ observe() if LANGFUSE_OBSERVE_AVAILABLE else lambda func: func
    async def run(
        self,
        user_level: UserLevel,
        user_query: str,
        conversation_history: list[dict[str, str]] = None,
        memory_context: str = None,
    ) -> str:
        """
        ë ˆë²¨ë³„ ì²´ì¸ ì‹¤í–‰
        """
        level_chain = self.get_chain(user_level)

        # Langfuse ë©”íƒ€ë°ì´í„° ì„¤ì • (ìµœìƒìœ„ í˜¸ì¶œ)
        if LANGFUSE_OBSERVE_AVAILABLE and self.langfuse_manager:
            self.langfuse_manager.update_current_trace(
                name="level_chain_run", input_data={"user_level": user_level.value, "service": "level_chain_service"}
            )

        # ë¹ˆ ì¿¼ë¦¬ ì²´í¬
        if not user_query or not user_query.strip():
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ìœ íš¨í•œ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."

        try:
            logger.info(f"ë ˆë²¨ë³„ ì²´ì¸ ì‹œì‘: level={user_level.value}")

            # LevelChainìœ¼ë¡œ ì²˜ë¦¬
            result = await level_chain.process(
                user_level=user_level,
                user_query=user_query,
                conversation_history=conversation_history,
                memory_context=memory_context,
            )

            final_response = result.get("final_response", "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")

            logger.info(f"ë ˆë²¨ë³„ ì²´ì¸ ì™„ë£Œ: level={user_level.value}, tools={result.get('tools_used', [])}")
            return final_response

        except Exception as e:
            logger.error(f"ë ˆë²¨ë³„ ì²´ì¸ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
