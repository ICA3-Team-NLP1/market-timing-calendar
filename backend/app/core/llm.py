"""
LLM ê³µí†µ ëª¨ë“ˆ - Provider Factoryì™€ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤
"""
import os
import logging
from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Optional

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

from app.core.config import settings

# Langfuse ì„í¬íŠ¸ (ì„ íƒì )
try:
    from langfuse.langchain import CallbackHandler
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False
    CallbackHandler = None

logger = logging.getLogger(__name__)


class LangfuseManager:
    """Langfuse ê´€ë ¨ ê³µí†µ ìœ í‹¸ë¦¬í‹°"""
    
    def __init__(self, service_name: str = "llm"):
        self.service_name = service_name
        self.handler: Optional[CallbackHandler] = None
        self._initialize_handler()
    
    def _initialize_handler(self) -> None:
        """Langfuse CallbackHandler ì´ˆê¸°í™”"""
        if not LANGFUSE_AVAILABLE:
            logger.info(f"[{self.service_name}] Langfuseê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ, ëª¨ë‹ˆí„°ë§ ë¹„í™œì„±í™”")
            return
        
        try:
            # í•„ìˆ˜ ì„¤ì •ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ Langfuse handler ìƒì„±
            if settings.LANGFUSE_PUBLIC_KEY and settings.LANGFUSE_SECRET_KEY:
                # Settingsì—ì„œ ì½ì€ ê°’ì„ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • (Langfuseê°€ ì½ì„ ìˆ˜ ìˆë„ë¡)
                self._set_environment_variables()
                
                # CallbackHandler ìƒì„± ë° í…ŒìŠ¤íŠ¸
                self.handler = CallbackHandler()
                logger.info(f"âœ… [{self.service_name}] Langfuse CallbackHandler ìƒì„± ì„±ê³µ: {type(self.handler)}")
                
            else:
                logger.info(f"[{self.service_name}] Langfuse í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ, ëª¨ë‹ˆí„°ë§ ë¹„í™œì„±í™”")
        except Exception as e:
            logger.warning(f"[{self.service_name}] Langfuse ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            logger.warning(f"[{self.service_name}] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    def _set_environment_variables(self) -> None:
        """Langfuse í™˜ê²½ë³€ìˆ˜ ì„¤ì •"""
        os.environ['LANGFUSE_PUBLIC_KEY'] = settings.LANGFUSE_PUBLIC_KEY
        os.environ['LANGFUSE_SECRET_KEY'] = settings.LANGFUSE_SECRET_KEY
        os.environ['LANGFUSE_HOST'] = settings.LANGFUSE_HOST
        
        logger.info(f"âœ… [{self.service_name}] Langfuse í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ: {settings.LANGFUSE_HOST}")
        logger.info(f"ğŸ”‘ [{self.service_name}] Public Key: {settings.LANGFUSE_PUBLIC_KEY[:15]}...")
    
    def get_callback_config(self) -> dict:
        """LLM í˜¸ì¶œì— ì‚¬ìš©í•  callback config ë°˜í™˜"""
        config = {}
        if self.handler:
            config["callbacks"] = [self.handler]
            logger.info(f"ğŸ¯ [{self.service_name}] Langfuse callback ì„¤ì •ë¨: {type(self.handler)}")
        else:
            logger.warning(f"âš ï¸ [{self.service_name}] Langfuse handlerê°€ ì—†ìŒ - ëª¨ë‹ˆí„°ë§ ë¶ˆê°€")
        return config
    
    def flush_events(self) -> None:
        """Langfuse ì´ë²¤íŠ¸ë¥¼ ì„œë²„ë¡œ ì „ì†¡ (Background ì‘ì—…ìš©)"""
        if not self.handler:
            return
            
        try:
            from langfuse import get_client
            client = get_client()
            if client:
                client.flush()
                logger.info(f"ğŸ“¤ [{self.service_name}] Langfuse ì´ë²¤íŠ¸ ì„œë²„ ì „ì†¡ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ [{self.service_name}] Langfuse flush ì‹¤íŒ¨: {e}")
    
    @property
    def is_available(self) -> bool:
        """Langfuse handlerê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸"""
        return self.handler is not None


class LLMProviderType(str, Enum):
    """LLM í”„ë¡œë°”ì´ë” íƒ€ì…"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"


class BaseLLMProvider(ABC):
    """LLM í”„ë¡œë°”ì´ë” ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def create_llm(self) -> Any:
        """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        pass


class OpenAIProvider(BaseLLMProvider):
    """OpenAI LLM í”„ë¡œë°”ì´ë”"""
    
    def __init__(self, model: str, temperature: float = 0, **kwargs):
        self.model = model
        self.temperature = temperature
        self.kwargs = kwargs
    
    def create_llm(self) -> ChatOpenAI:
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì • (background ì„œë¹„ìŠ¤ì™€ í˜¸í™˜ì„± ìœ ì§€)
        if settings.OPENAI_API_KEY:
            os.environ["OPENAI_API_KEY"] = settings.OPENAI_API_KEY
        
        return ChatOpenAI(
            model=self.model, 
            temperature=self.temperature,
            api_key=settings.OPENAI_API_KEY,
            **self.kwargs
        )


class AnthropicProvider(BaseLLMProvider):
    """Anthropic LLM í”„ë¡œë°”ì´ë”"""
    
    def __init__(self, model: str, temperature: float = 0, **kwargs):
        self.model = model
        self.temperature = temperature
        self.kwargs = kwargs
    
    def create_llm(self) -> ChatAnthropic:
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì • (background ì„œë¹„ìŠ¤ì™€ í˜¸í™˜ì„± ìœ ì§€)
        if settings.ANTHROPIC_API_KEY:
            os.environ["ANTHROPIC_API_KEY"] = settings.ANTHROPIC_API_KEY
            
        return ChatAnthropic(
            model=self.model, 
            temperature=self.temperature,
            api_key=settings.ANTHROPIC_API_KEY,
            **self.kwargs
        )


class LLMFactory:
    """LLM íŒ©í† ë¦¬ í´ë˜ìŠ¤ - ê³µí†µ LLM ìƒì„± ë¡œì§"""
    
    _providers = {
        LLMProviderType.OPENAI: OpenAIProvider,
        LLMProviderType.ANTHROPIC: AnthropicProvider,
    }
    
    @classmethod
    def create_provider(
        cls, 
        provider_type: str, 
        model: str, 
        temperature: float = 0,
        **kwargs
    ) -> BaseLLMProvider:
        """LLM í”„ë¡œë°”ì´ë” ìƒì„±"""
        # provider_typeì´ Noneì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ì„¤ì •ì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not provider_type:
            provider_type = settings.ACTIVE_LLM_PROVIDER or "openai"
            logger.info(f"provider_typeì´ Noneì´ë¯€ë¡œ ì„¤ì •ê°’ ì‚¬ìš©: {provider_type}")
        
        # ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì¸ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if provider_type.lower() not in [p.value for p in LLMProviderType]:
            logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM provider: {provider_type}. OpenAIë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            provider_type = LLMProviderType.OPENAI
        
        # modelì´ Noneì¸ ê²½ìš° ì„¤ì •ì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not model:
            model = settings.ACTIVE_LLM_MODEL
            logger.info(f"modelì´ Noneì´ë¯€ë¡œ ì„¤ì •ê°’ ì‚¬ìš©: {model}")
        
        provider_class = cls._providers.get(LLMProviderType(provider_type.lower()))
        if not provider_class:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM provider: {provider_type}")
        
        return provider_class(model, temperature, **kwargs)
    
    @classmethod
    def create_llm(
        cls,
        provider_type: str = None,
        model: str = None, 
        temperature: float = 0,
        **kwargs
    ) -> Any:
        """LLM ì¸ìŠ¤í„´ìŠ¤ ì§ì ‘ ìƒì„± (í¸ì˜ ë©”ì„œë“œ)"""
        provider_type = provider_type or settings.ACTIVE_LLM_PROVIDER or "openai"
        model = model or settings.ACTIVE_LLM_MODEL
        
        provider = cls.create_provider(provider_type, model, temperature, **kwargs)
        return provider.create_llm() 