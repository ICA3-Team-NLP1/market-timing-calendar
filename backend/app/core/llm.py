"""
LLM ê³µí†µ ëª¨ë“ˆ - Provider Factoryì™€ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤
"""
import os
import logging
import os
import uuid
from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Optional, Dict

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

from .config import settings

# Langfuse ì„í¬íŠ¸ (ì„ íƒì )
try:
    from langfuse.langchain import CallbackHandler
    from langfuse import observe, get_client
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False
    CallbackHandler = None
    observe = None
    get_client = None

logger = logging.getLogger(__name__)


class LangfuseManager:
    """Langfuse ê´€ë ¨ ê³µí†µ ìœ í‹¸ë¦¬í‹° (Single Responsibility)"""
    
    def __init__(self, service_name: str = "llm", user_id: Optional[str] = None, session_id: Optional[str] = None):
        self.service_name = service_name
        self.user_id = user_id
        # Backgroundì—ì„œ Celery task_id ìë™ ê°ì§€
        if session_id is None and service_name == "background_etl":
            self.session_id = self._get_celery_task_id()
        else:
            self.session_id = session_id or str(uuid.uuid4())
        self.handler: Optional[CallbackHandler] = None
        self._initialize_handler()
    
    def _get_celery_task_id(self) -> str:
        """Celery task_idë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ UUID ìƒì„±"""
        try:
            # ë°©ë²• 1: celery.current_task ì‚¬ìš©
            import celery
            if hasattr(celery, 'current_task') and celery.current_task:
                if hasattr(celery.current_task, 'request') and celery.current_task.request:
                    task_id = celery.current_task.request.id
                    if task_id:
                        logger.info(f"âœ… Celery task_id ê°ì§€: {task_id}")
                        return task_id
            
            # ë°©ë²• 2: ì§ì ‘ import ì‹œë„
            try:
                from celery import current_task
                if current_task and hasattr(current_task, 'request') and current_task.request:
                    task_id = current_task.request.id
                    if task_id:
                        logger.info(f"âœ… Celery current_task.task_id ê°ì§€: {task_id}")
                        return task_id
            except ImportError:
                pass
                
        except Exception as e:
            logger.warning(f"âš ï¸ Celery task_id ê°ì§€ ì‹¤íŒ¨: {e}")
        
        # Fallback: UUID ìƒì„±
        fallback_id = str(uuid.uuid4())
        logger.info(f"ğŸ”„ Celery task_id ê°ì§€ ì‹¤íŒ¨, UUID ì‚¬ìš©: {fallback_id}")
        return fallback_id
    
    @staticmethod
    def create_for_app(user=None, session_id: Optional[str] = None) -> 'LangfuseManager':
        """Appì—ì„œ ì‚¬ìš©ì ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ LangfuseManager ìƒì„±"""
        user_id = None
        metadata = {}
        
        if user:
            # uidê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ email_name ì¡°í•© ì‚¬ìš©
            if hasattr(user, 'uid') and user.uid:
                user_id = user.uid
            elif hasattr(user, 'email') and hasattr(user, 'name'):
                user_id = f"{user.email}_{user.name}"
            
            # ë©”íƒ€ë°ì´í„°ì— ì‚¬ìš©ì ì •ë³´ ì¶”ê°€
            if hasattr(user, 'email'):
                metadata["user_email"] = user.email
            if hasattr(user, 'name'):
                metadata["user_name"] = user.name
        
        return LangfuseManager(
            service_name="backend_chatbot",
            user_id=user_id,
            session_id=session_id
        )
    
    @staticmethod
    def create_for_background(task_metadata: Optional[Dict[str, Any]] = None) -> 'LangfuseManager':
        """Backgroundì—ì„œ ì‚¬ìš©í•  LangfuseManager ìƒì„±"""
        metadata = {
            "service": "background_etl",
            "task_type": "etl_inference"
        }
        if task_metadata:
            metadata.update(task_metadata)
        
        return LangfuseManager(
            service_name="background_etl",
            user_id="background-module",
            session_id=None  # Celery task_id ìë™ ê°ì§€
        )
    
    def _initialize_handler(self) -> None:
        """Langfuse CallbackHandler ì´ˆê¸°í™”"""
        if not LANGFUSE_AVAILABLE:
            logger.info(f"[{self.service_name}] Langfuseê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ, ëª¨ë‹ˆí„°ë§ ë¹„í™œì„±í™”")
            return
        
        try:
            # í•„ìˆ˜ ì„¤ì •ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ Langfuse handler ìƒì„±
            if settings.LANGFUSE_PUBLIC_KEY and settings.LANGFUSE_SECRET_KEY:
                # Settingsì—ì„œ ì½ì€ ê°’ì„ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • (Langfuseê°€ ì½ì„ ìˆ˜ ìˆë„ë¡)
                self._set_environment_variables()
                
                # CallbackHandler ìƒì„± ë° í…ŒìŠ¤íŠ¸
                self.handler = CallbackHandler()
                logger.info(f"âœ… [{self.service_name}] Langfuse CallbackHandler ìƒì„± ì„±ê³µ: {type(self.handler)}")
                
            else:
                logger.info(f"[{self.service_name}] Langfuse í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ, ëª¨ë‹ˆí„°ë§ ë¹„í™œì„±í™”")
        except Exception as e:
            logger.warning(f"[{self.service_name}] Langfuse ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            logger.warning(f"[{self.service_name}] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    def _set_environment_variables(self) -> None:
        """Langfuse í™˜ê²½ë³€ìˆ˜ ì„¤ì •"""
        os.environ['LANGFUSE_PUBLIC_KEY'] = settings.LANGFUSE_PUBLIC_KEY
        os.environ['LANGFUSE_SECRET_KEY'] = settings.LANGFUSE_SECRET_KEY
        os.environ['LANGFUSE_HOST'] = settings.LANGFUSE_HOST
        
        logger.info(f"âœ… [{self.service_name}] Langfuse í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ: {settings.LANGFUSE_HOST}")
        logger.info(f"ğŸ”‘ [{self.service_name}] Public Key: {settings.LANGFUSE_PUBLIC_KEY[:15]}...")
    
    def get_callback_config(self, metadata: Optional[Dict[str, Any]] = None) -> dict:
        """LLM í˜¸ì¶œì— ì‚¬ìš©í•  callback config ë°˜í™˜"""
        config = {}
        if self.handler:
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° êµ¬ì„±
            base_metadata = {
                "service": self.service_name,
            }
            
            # Langfuse ê³µì‹ ë¬¸ì„œì— ë”°ë¥¸ session_idì™€ user_id ì„¤ì •
            if self.session_id:
                base_metadata["langfuse_session_id"] = self.session_id
            if self.user_id:
                base_metadata["langfuse_user_id"] = self.user_id
                
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° ë³‘í•©
            if metadata:
                base_metadata.update(metadata)
            
            config["callbacks"] = [self.handler]
            config["metadata"] = base_metadata
            
            logger.info(f"ğŸ¯ [{self.service_name}] Langfuse callback ì„¤ì •ë¨: session_id={self.session_id}, user_id={self.user_id}")
        else:
            logger.warning(f"âš ï¸ [{self.service_name}] Langfuse handlerê°€ ì—†ìŒ - ëª¨ë‹ˆí„°ë§ ë¶ˆê°€")
        return config
    
    def update_current_trace(self, name: str = None, input_data: Dict = None, output_data: Dict = None) -> None:
        """í˜„ì¬ traceë¥¼ ì—…ë°ì´íŠ¸ (ì˜¬ë°”ë¥¸ ë°©ì‹)"""
        if not LANGFUSE_AVAILABLE or not get_client:
            return
            
        try:
            langfuse = get_client()
            if langfuse:
                # trace ì—…ë°ì´íŠ¸
                if name:
                    langfuse.update_current_trace(name=name)
                if self.user_id:
                    langfuse.update_current_trace(user_id=self.user_id)
                if self.session_id:
                    langfuse.update_current_trace(session_id=self.session_id)
                if input_data:
                    langfuse.update_current_trace(input=input_data)
                if output_data:
                    langfuse.update_current_trace(output=output_data)
                    
                logger.info(f"âœ… [{self.service_name}] Trace ì—…ë°ì´íŠ¸ ì™„ë£Œ: user_id={self.user_id}, session_id={self.session_id}")
        except Exception as e:
            logger.warning(f"âš ï¸ [{self.service_name}] Trace ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def flush_events(self) -> None:
        """Langfuse ì´ë²¤íŠ¸ë¥¼ ì„œë²„ë¡œ ì „ì†¡ (Background ì‘ì—…ìš©)"""
        if not self.handler:
            return
            
        try:
            if get_client:
                client = get_client()
                if client:
                    client.flush()
                    logger.info(f"ğŸ“¤ [{self.service_name}] Langfuse ì´ë²¤íŠ¸ ì„œë²„ ì „ì†¡ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ [{self.service_name}] Langfuse flush ì‹¤íŒ¨: {e}")
    
    @property
    def is_available(self) -> bool:
        """Langfuse handlerê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸"""
        return self.handler is not None


class LLMProviderType(str, Enum):
    """LLM í”„ë¡œë°”ì´ë” íƒ€ì…"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"


class BaseLLMProvider(ABC):
    """LLM í”„ë¡œë°”ì´ë” ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def create_llm(self) -> Any:
        """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        pass


class OpenAIProvider(BaseLLMProvider):
    """OpenAI LLM í”„ë¡œë°”ì´ë”"""
    
    def __init__(self, model: str, temperature: float = 0, **kwargs):
        self.model = model
        self.temperature = temperature
        self.kwargs = kwargs
    
    def create_llm(self) -> ChatOpenAI:
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì • (background ì„œë¹„ìŠ¤ì™€ í˜¸í™˜ì„± ìœ ì§€)
        if settings.OPENAI_API_KEY:
            os.environ["OPENAI_API_KEY"] = settings.OPENAI_API_KEY
        
        return ChatOpenAI(
            model=self.model, 
            temperature=self.temperature,
            api_key=settings.OPENAI_API_KEY,
            **self.kwargs
        )


class AnthropicProvider(BaseLLMProvider):
    """Anthropic LLM í”„ë¡œë°”ì´ë”"""
    
    def __init__(self, model: str, temperature: float = 0, **kwargs):
        self.model = model
        self.temperature = temperature
        self.kwargs = kwargs
    
    def create_llm(self) -> ChatAnthropic:
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì • (background ì„œë¹„ìŠ¤ì™€ í˜¸í™˜ì„± ìœ ì§€)
        if settings.ANTHROPIC_API_KEY:
            os.environ["ANTHROPIC_API_KEY"] = settings.ANTHROPIC_API_KEY
            
        return ChatAnthropic(
            model=self.model, 
            temperature=self.temperature,
            api_key=settings.ANTHROPIC_API_KEY,
            **self.kwargs
        )


class LLMFactory:
    """LLM íŒ©í† ë¦¬ í´ë˜ìŠ¤ - ê³µí†µ LLM ìƒì„± ë¡œì§"""
    
    _providers = {
        LLMProviderType.OPENAI: OpenAIProvider,
        LLMProviderType.ANTHROPIC: AnthropicProvider,
    }
    
    @classmethod
    def create_provider(
        cls, 
        provider_type: str, 
        model: str, 
        temperature: float = 0,
        **kwargs
    ) -> BaseLLMProvider:
        """LLM í”„ë¡œë°”ì´ë” ìƒì„±"""
        # provider_typeì´ Noneì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ì„¤ì •ì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not provider_type:
            provider_type = settings.ACTIVE_LLM_PROVIDER or "openai"
            logger.info(f"provider_typeì´ Noneì´ë¯€ë¡œ ì„¤ì •ê°’ ì‚¬ìš©: {provider_type}")
        
        # ì§€ì›í•˜ì§€ ì•ŠëŠ” providerì¸ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if provider_type.lower() not in [p.value for p in LLMProviderType]:
            logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM provider: {provider_type}. OpenAIë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            provider_type = LLMProviderType.OPENAI
        
        # modelì´ Noneì¸ ê²½ìš° ì„¤ì •ì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not model:
            model = settings.ACTIVE_LLM_MODEL
            logger.info(f"modelì´ Noneì´ë¯€ë¡œ ì„¤ì •ê°’ ì‚¬ìš©: {model}")
        
        provider_class = cls._providers.get(LLMProviderType(provider_type.lower()))
        if not provider_class:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM provider: {provider_type}")
        
        return provider_class(model, temperature, **kwargs)
    
    @classmethod
    def create_llm(
        cls,
        provider_type: str = None,
        model: str = None, 
        temperature: float = 0,
        **kwargs
    ) -> Any:
        """LLM ì¸ìŠ¤í„´ìŠ¤ ì§ì ‘ ìƒì„± (í¸ì˜ ë©”ì„œë“œ)"""
        provider_type = provider_type or settings.ACTIVE_LLM_PROVIDER or "openai"
        model = model or settings.ACTIVE_LLM_MODEL
        
        provider = cls.create_provider(provider_type, model, temperature, **kwargs)
        return provider.create_llm() 