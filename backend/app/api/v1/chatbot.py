from __future__ import annotations

import logging
from typing import List

from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session

from app.api.deps import get_or_create_user, get_session
from app.models.users import Users
from app.utils.llm_client import LLMClient
from app.core.prompts import SYSTEM_PROMPTS, EVENT_EXPLAIN_PROMPTS
from app.constants import UserLevel, ChatMessageRole
from app.crud.crud_events import crud_events
from app.crud.crud_chat import crud_chat_sessions
from app.schemas.chatbot import *
from app.schemas.chat import *
from app.services.filter_service import FilterService
from app.services.mem0_service import mem0_service
from app.services.mem0_client import mem0_client
from app.core.langfuse_factory import LangfuseFactory
from app.utils.session import resolve_session_id
from app.services.level_chain import LevelChainService

logger = logging.getLogger(__name__)

# Langfuse observe ë°ì½”ë ˆì´í„° ì„í¬íŠ¸
try:
    from langfuse import observe

    LANGFUSE_OBSERVE_AVAILABLE = True
except ImportError:
    LANGFUSE_OBSERVE_AVAILABLE = False
    observe = None

chatbot_router = APIRouter()


def _build_messages(
    level: UserLevel, history: List[ChatMessage], question: str, memory_context: str = None
) -> List[dict]:
    """ëŒ€í™” ë©”ì‹œì§€ êµ¬ì„±

    Args:
        level: ì‚¬ìš©ì ë ˆë²¨
        history: ì´ì „ ëŒ€í™” ë‚´ì—­
        question: ìƒˆë¡œìš´ ì§ˆë¬¸

    Returns:
        LLMì— ì „ë‹¬í•  ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    """
    system_prompt = SYSTEM_PROMPTS.get(level, SYSTEM_PROMPTS[UserLevel.BEGINNER])
    if memory_context:
        system_prompt += f"\n\n[ì´ì „ ëŒ€í™” ê¸°ì–µ]\n{memory_context}"
    messages = [{"role": "system", "content": system_prompt}]
    messages += [msg.dict() for msg in history]
    messages.append({"role": "user", "content": question})
    return messages


@ observe() if LANGFUSE_OBSERVE_AVAILABLE else lambda func: func
@chatbot_router.post("/conversation")
async def conversation(
    req: ConversationRequest,
    use_filter: bool = Query(True, description="í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€"),
    use_level_chain: bool = Query(True, description="ë ˆë²¨ë³„ ì²´ì¸ ì‚¬ìš© ì—¬ë¶€"),
    is_mem0_api: bool = True,
    chunk_size: int = Query(50, description="ì‘ë‹µ Chunk Size"),
    db_user: Users = Depends(get_or_create_user),
    session: Session = Depends(get_session),
):
    """ëŒ€í™” ë‚´ì—­ ê¸°ë°˜ ì§ˆë¬¸ ì²˜ë¦¬

    ì‚¬ìš©ìì˜ ì´ì „ ëŒ€í™” ë‚´ì—­ì„ ë°”íƒ•ìœ¼ë¡œ ì—°ì†ì ì¸ ì§ˆë¬¸ì— ëŒ€í•´ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.
    ë ˆë²¨ë³„ LLM ì²´ì¸(LangGraph)ì„ í†µí•´ ì‚¬ìš©ì ë ˆë²¨ì— ë”°ë¥¸ ì°¨ë³„í™”ëœ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        req: ëŒ€í™” ìš”ì²­ (ëŒ€í™” ë‚´ì—­, ì§ˆë¬¸, ì•ˆì „ ìˆ˜ì¤€)
        use_filter: í•„í„°ë§ ì ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        use_level_chain: ë ˆë²¨ë³„ ì²´ì¸ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        is_mem0_api: Mem0 API ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        chunk_size: ì‘ë‹µ Chunk Size
        db_user: í˜„ì¬ ì‚¬ìš©ì ì •ë³´
        session: DB session
    """
    try:
        # ì„¸ì…˜ ì²˜ë¦¬ (ìƒˆ ì„¸ì…˜ì´ë©´ ìƒì„±, ê¸°ì¡´ ì„¸ì…˜ì´ë©´ ì¡°íšŒ)
        user_uid = db_user.uid
        user_level = db_user.level
        session_id = resolve_session_id(req.session_id)

        chat_session = crud_chat_sessions.get(session=session, session_id=session_id)
        if not chat_session:
            # ìƒˆë¡œìš´ ì„¸ì…˜ ìƒì„±
            chat_session = crud_chat_sessions.create(
                session=session,
                obj_in=ChatSessionCreate(
                    user_id=db_user.id,
                    session_id=session_id,
                ),
            )
            session.commit()
            req.session_id = chat_session.session_id

        # mem0ì—ì„œ ê´€ë ¨ ë©”ëª¨ë¦¬ ê²€ìƒ‰ (use_memoryê°€ Trueì¸ ê²½ìš°)
        mem0_provider = mem0_client if is_mem0_api else mem0_service
        memory_context = None
        if req.use_memory:
            relevant_memories = await mem0_provider.search_relevant_memories(user_id=db_user.uid, query=req.question)

            if relevant_memories:
                memory_context = mem0_provider.build_memory_context(relevant_memories)
                logger.debug(f"ğŸ§  ê´€ë ¨ ë©”ëª¨ë¦¬ {len(relevant_memories)}ê°œ ë°œê²¬")

        langfuse_manager = LangfuseFactory.create_app_manager(user=db_user, session_id=session_id)

        async def stream():
            full_response = ""
            try:
                # ë ˆë²¨ë³„ ì²´ì¸ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ë¶„ê¸°
                if use_level_chain:
                    level_chain_service = LevelChainService(user=db_user, langfuse_manager=langfuse_manager)
                    final_response = await level_chain_service.run(
                        user_level=user_level,
                        user_query=req.question,
                        conversation_history=[msg.dict() for msg in req.history],
                        memory_context=memory_context,
                    )

                    # í•„í„°ë§ ì ìš©
                    if use_filter:
                        filter_service = FilterService(user=db_user, langfuse_manager=langfuse_manager)
                        filter_result = await filter_service.filter_response(final_response, req.safety_level)
                        final_response = filter_result["content"]

                    full_response = final_response

                    # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ì „ì†¡ (ì²­í¬ ë‹¨ìœ„)
                    for i in range(0, len(final_response), chunk_size):
                        chunk = final_response[i : i + chunk_size]
                        yield chunk

                else:
                    # ê¸°ì¡´ ë°©ì‹
                    llm_client = LLMClient(user=db_user, langfuse_manager=langfuse_manager)
                    messages = _build_messages(user_level, req.history, req.question, memory_context)
                    if use_filter:
                        async for chunk in llm_client.stream_chat_with_filter(
                            messages, safety_level=req.safety_level, chunk_size=chunk_size
                        ):
                            full_response += chunk
                            yield chunk
                    else:
                        async for chunk in llm_client.stream_chat(messages):
                            full_response += chunk
                            yield chunk

                logger.debug(f"ğŸ¯ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {len(full_response)}ê¸€ì")

                # ì„¸ì…˜ ë©”ì‹œì§€ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸ (user + assistant = 2)
                crud_chat_sessions.increment_message_count(session=session, session_id=session_id, count=2)

                # mem0ì— ëŒ€í™” ë‚´ìš© ì¶”ê°€
                if req.use_memory:
                    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
                    messages = [
                        {"role": ChatMessageRole.user, "content": req.question},
                        {"role": ChatMessageRole.assistant, "content": full_response},
                    ]
                    await mem0_provider.add_conversation_message(
                        user_id=user_uid, messages=messages, session_id=session_id
                    )
                    logger.debug("ğŸ§  mem0ì— ëŒ€í™” ë‚´ìš© ì €ì¥ ì™„ë£Œ")

                session.commit()

                # ì¶”ê°€ ë©”íƒ€ë°ì´í„°ë¥¼ í—¤ë”ë¡œ ì „ì†¡
                yield f"\n\n<!-- SESSION_ID: {session_id} -->"

            except Exception as e:
                logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {e}")
                session.rollback()
                yield f"\n\nì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        return StreamingResponse(stream(), media_type="text/plain")

    except Exception as e:
        raise HTTPException(status_code=500, detail="ëŒ€í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@ observe() if LANGFUSE_OBSERVE_AVAILABLE else lambda func: func
@chatbot_router.post("/event/explain")
async def explain_event(
    req: EventExplainRequest,
    use_filter: bool = Query(True, description="í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€"),
    db_user: Users = Depends(get_or_create_user),
    db: Session = Depends(get_session),
):
    """ê¸ˆìœµ ì´ë²¤íŠ¸ ì„¤ëª…

    íŠ¹ì • ê¸ˆìœµ ì´ë²¤íŠ¸ë‚˜ ê²½ì œ ì§€í‘œì— ëŒ€í•œ ì„¤ëª…ì„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.
    ì‚¬ìš©ì ë ˆë²¨ì— ë”°ë¼ ì ì ˆí•œ ê¹Šì´ì˜ ì„¤ëª…ì„ ì œê³µí•˜ë©°, ì»¨í…ì¸  í•„í„°ë§ì„ ì ìš©í•©ë‹ˆë‹¤.

    Args:
        req: ì´ë²¤íŠ¸ ì„¤ëª… ìš”ì²­ (Events í…Œì´ë¸”ì˜ id, ì•ˆì „ ìˆ˜ì¤€)
        use_filter: í•„í„°ë§ ì ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        db_user: í˜„ì¬ ì‚¬ìš©ì ì •ë³´
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
    """
    try:
        # session_id ì²˜ë¦¬ (conversationê³¼ ë™ì¼í•˜ê²Œ)
        session_id = resolve_session_id(getattr(req, "session_id", None))

        # idë¡œ ì´ë²¤íŠ¸ ì¡°íšŒ
        event = crud_events.get(db, id=req.id)
        if not event:
            raise HTTPException(status_code=404, detail=f"Event with id '{req.id}' not found")

        # ì´ë²¤íŠ¸ ì •ë³´ë¥¼ í¬ë§·íŒ…í•˜ì—¬ LLMì—ê²Œ ì „ë‹¬
        event_context = f"""ì´ë²¤íŠ¸ ì •ë³´:
- ì œëª©: {event.title}
- ì„¤ëª…: {event.description}
- ë‚ ì§œ: {event.date}
- ì˜í–¥ë„: {event.impact}
- ì¶œì²˜: {event.source}
- Release ID: {event.release_id}

ìœ„ ê²½ì œ ì§€í‘œ/ì´ë²¤íŠ¸ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."""

        system_prompt = EVENT_EXPLAIN_PROMPTS.get(db_user.level, EVENT_EXPLAIN_PROMPTS[UserLevel.BEGINNER])
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": event_context},
        ]

        langfuse_manager = LangfuseFactory.create_app_manager(user=db_user, session_id=session_id)
        llm_client = LLMClient(user=db_user, langfuse_manager=langfuse_manager)

        async def stream():
            if use_filter:
                # í•„í„°ë§ ì ìš©ëœ ìŠ¤íŠ¸ë¦¬ë°
                async for chunk in llm_client.stream_chat_with_filter(messages, safety_level=req.safety_level):
                    yield chunk
            else:
                # ê¸°ì¡´ ë°©ì‹ (í•„í„°ë§ ì—†ìŒ)
                async for chunk in llm_client.stream_chat(messages):
                    yield chunk

        return StreamingResponse(stream(), media_type="text/plain")

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail="ì´ë²¤íŠ¸ ì„¤ëª… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@ observe() if LANGFUSE_OBSERVE_AVAILABLE else lambda func: func
@chatbot_router.post("/safety/check")
async def check_content_safety(req: SafetyCheckRequest, db_user: Users = Depends(get_or_create_user)):
    """ì»¨í…ì¸  ì•ˆì „ì„± ê²€ì‚¬

    ì£¼ì–´ì§„ ì»¨í…ì¸ ì˜ ì•ˆì „ì„±ì„ ê²€ì‚¬í•˜ê³  ìœ„í—˜ ìš”ì†Œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    ì‹¤ì œ í•„í„°ë§ì€ ìˆ˜í–‰í•˜ì§€ ì•Šê³  ë¶„ì„ ê²°ê³¼ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        req: ì•ˆì „ì„± ê²€ì‚¬ ìš”ì²­
        db_user: í˜„ì¬ ì‚¬ìš©ì ì •ë³´

    Returns:
        {
            "is_safe": true/false,
            "safety_score": 0.85,
            "risk_categories": ["investment_advice", "guaranteed_profit"],
            "filter_reason": "ìƒì„¸í•œ ë¶„ì„ ê²°ê³¼",
            "analysis_only": true
        }
    """
    try:

        llm_client = LLMClient(user=db_user)
        result = await llm_client.check_content_safety(req.content)

        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail="ì•ˆì „ì„± ê²€ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@chatbot_router.get("/filter/status")
async def get_filter_status(db_user: Users = Depends(get_or_create_user)):
    """í•„í„°ë§ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ

    í˜„ì¬ í•„í„°ë§ ì‹œìŠ¤í…œì˜ ì„¤ì •ê³¼ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        db_user: í˜„ì¬ ì‚¬ìš©ì ì •ë³´

    Returns:
        {
            "enabled": true,
            "safety_level": "strict",
            "max_retries": 3,
            "filter_model": "gpt-4",
            "filter_provider": "openai"
        }
    """
    try:
        filter_service = FilterService()
        status = filter_service.get_filter_stats()

        return status

    except Exception as e:
        raise HTTPException(status_code=500, detail="í•„í„° ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@chatbot_router.get("/memory/status", response_model=MemoryStatusResponse)
async def get_memory_status(is_mem0_api: bool = True, db_user: Users = Depends(get_or_create_user)):
    """ì‚¬ìš©ìì˜ mem0 ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        mem0_provider = mem0_client if is_mem0_api else mem0_service
        memories = await mem0_provider.get_user_memories(db_user.uid)

        return MemoryStatusResponse(
            user_id=db_user.uid, memory_count=len(memories), memories=memories[:10]  # ìµœê·¼ 10ê°œë§Œ í‘œì‹œ
        )

    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@chatbot_router.delete("/memory/reset", response_model=MemoryResetResponse)
async def reset_user_memory(is_mem0_api: bool = True, db_user: Users = Depends(get_or_create_user)):
    """ì‚¬ìš©ìì˜ mem0 ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
    try:
        mem0_provider = mem0_client if is_mem0_api else mem0_service
        result = await mem0_provider.reset_user_memory(db_user.uid)

        if result["success"]:
            return MemoryResetResponse(success=True, message=result["message"])
        else:
            raise HTTPException(status_code=500, detail=result["error"])

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
