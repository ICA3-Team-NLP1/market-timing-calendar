#!/usr/bin/env python3
"""Background LLM Service - ETL ì‘ì—…ìš© LLM ì¶”ìƒí™”"""

import logging
import asyncio
import time
from typing import Dict, List, Optional, Any
from enum import Enum

from langchain_core.prompts import ChatPromptTemplate

from app.core.llm import LLMFactory, BaseLLMProvider, LangfuseManager

# Langfuse observe ë°ì½”ë ˆì´í„° ì„í¬íŠ¸
try:
    from langfuse import observe
    LANGFUSE_OBSERVE_AVAILABLE = True
except ImportError:
    LANGFUSE_OBSERVE_AVAILABLE = False
    observe = None

logger = logging.getLogger(__name__)


class InferenceType(str, Enum):
    """ì¶”ë¡  íƒ€ì… (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    IMPACT = "impact"
    LEVEL = "level"
    DESCRIPTION_KO = "description_ko"


class BackgroundLLMService:
    """Background ETL ì‘ì—…ìš© LLM ì„œë¹„ìŠ¤"""

    def __init__(self, task_id: str = None, langfuse_manager: Optional[LangfuseManager] = None):
        # core ëª¨ë“ˆì˜ LLMFactory ì‚¬ìš© - ê³µí†µ ë¡œì§ ì¬ì‚¬ìš©
        self.llm = LLMFactory.create_llm()
        # Langfuse Manager ì´ˆê¸°í™” (ì˜ì¡´ì„± ì£¼ì… ë˜ëŠ” ê¸°ë³¸ ìƒì„±)
        self.langfuse_manager = langfuse_manager or LangfuseManager.create_for_background()
        
        # Backgroundì—ì„œ Celery task_id ìë™ ê°ì§€
        if task_id:
            self.langfuse_manager.session_id = task_id

    def _create_chain(self, prompt_template: str):
        """Chain ìƒì„± ê³µí†µ ë¡œì§"""
        prompt = ChatPromptTemplate.from_template(prompt_template)
        return prompt | self.llm

    @observe() if LANGFUSE_OBSERVE_AVAILABLE else lambda func: func
    async def process_with_llm_observed(self, prompt_template: str, input_data: Dict[str, Any]) -> str:
        """@observe() ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•œ LLM ì²˜ë¦¬"""
        if LANGFUSE_OBSERVE_AVAILABLE and self.langfuse_manager:
            # trace ë©”íƒ€ë°ì´í„° ì„¤ì •
            self.langfuse_manager.update_current_trace(
                name="background_llm_process",
                input_data={"prompt_template": prompt_template, "input_data": input_data, "service": "background_etl"}
            )

        chain = self._create_chain(prompt_template)
        config = self.langfuse_manager.get_callback_config()

        logger.info(f"ğŸš€ Background LLM ì²˜ë¦¬ ì‹œì‘: {len(input_data)}ê°œ ì…ë ¥")
        logger.info(f"ğŸ‘¤ User ID: {self.langfuse_manager.user_id}, Session ID: {self.langfuse_manager.session_id}")

        result = await chain.ainvoke(input_data, config=config)

        if LANGFUSE_OBSERVE_AVAILABLE and self.langfuse_manager:
            self.langfuse_manager.update_current_trace(output_data={"status": "completed"})

        logger.info(f"ğŸ“Š Background LLM ì²˜ë¦¬ ì™„ë£Œ - Langfuse ì¶”ì ë¨")

        return result.content if hasattr(result, "content") else str(result)

    async def process_with_llm(self, prompt_template: str, input_data: Dict[str, Any]) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì²˜ë¦¬"""
        # @observe() ë°ì½”ë ˆì´í„°ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ìƒˆë¡œìš´ ë©”ì„œë“œ ì‚¬ìš©
        if LANGFUSE_OBSERVE_AVAILABLE:
            return await self.process_with_llm_observed(prompt_template, input_data)

        # ê¸°ì¡´ ë°©ì‹ (fallback)
        chain = self._create_chain(prompt_template)
        config = self.langfuse_manager.get_callback_config()

        logger.info(f"ğŸš€ Background LLM ì²˜ë¦¬ ì‹œì‘: {len(input_data)}ê°œ ì…ë ¥")
        logger.info(f"ğŸ‘¤ User ID: {self.langfuse_manager.user_id}, Session ID: {self.langfuse_manager.session_id}")

        result = await chain.ainvoke(input_data, config=config)

        logger.info(f"ğŸ“Š Background LLM ì²˜ë¦¬ ì™„ë£Œ - Langfuse ì¶”ì ë¨")

        return result.content if hasattr(result, "content") else str(result)

    async def process_fred_data(self, fred_data: Dict[str, Any]) -> Dict[str, Any]:
        """FRED ë°ì´í„°ë¥¼ LLMìœ¼ë¡œ ë¶„ì„"""
        prompt_template = """
        ë‹¤ìŒ ê²½ì œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
        
        ë°ì´í„°: {data}
        
        ë¶„ì„ ìš”ì²­: {analysis_request}
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        - ì£¼ìš” íŠ¸ë Œë“œ: 
        - ì‹œì¥ ì˜í–¥: 
        - íˆ¬ìì ê´€ì : 
        """

        input_data = {
            "data": str(fred_data),
            "analysis_request": "ê²½ì œ ì§€í‘œì˜ í˜„ì¬ ìƒíƒœì™€ í–¥í›„ ì „ë§ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
        }

        result = await self.process_with_llm(prompt_template, input_data)
        
        return {
            "analysis": result,
            "raw_data": fred_data,
            "processed_at": "2024-01-01"  # ì‹¤ì œë¡œëŠ” í˜„ì¬ ì‹œê°„ ì‚¬ìš©
        }

    async def process_event_data(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì´ë²¤íŠ¸ ë°ì´í„°ë¥¼ LLMìœ¼ë¡œ ë¶„ì„"""
        prompt_template = """
        ë‹¤ìŒ ì´ë²¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œì¥ ì˜í–¥ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”:
        
        ì´ë²¤íŠ¸: {event_data}
        
        ë¶„ì„ ìš”ì²­: {analysis_request}
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        - ì´ë²¤íŠ¸ ì¤‘ìš”ë„: 
        - ì‹œì¥ ì˜í–¥ë„: 
        - íˆ¬ìì ì£¼ì˜ì‚¬í•­: 
        """

        input_data = {
            "event_data": str(event_data),
            "analysis_request": "ì´ ì´ë²¤íŠ¸ê°€ ì‹œì¥ì— ë¯¸ì¹  ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
        }

        result = await self.process_with_llm(prompt_template, input_data)
        
        return {
            "analysis": result,
            "raw_data": event_data,
            "processed_at": "2024-01-01"  # ì‹¤ì œë¡œëŠ” í˜„ì¬ ì‹œê°„ ì‚¬ìš©
        }

    def flush_events(self):
        """Langfuse ì´ë²¤íŠ¸ë¥¼ ì„œë²„ë¡œ ì „ì†¡"""
        if self.langfuse_manager:
            self.langfuse_manager.flush_events()
            logger.info(f"ğŸ“¤ Langfuse ì´ë²¤íŠ¸ ì„œë²„ ì „ì†¡ ì™„ë£Œ")


# ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ LLMInferenceService ì–´ëŒ‘í„°
class LLMInferenceService:
    """ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ì–´ëŒ‘í„° í´ë˜ìŠ¤"""

    def __init__(self, background_service: BackgroundLLMService = None):
        self.background_service = background_service or BackgroundLLMService()

    def infer(self, inference_type: InferenceType, release_name: str, series_info: Dict[str, Any]) -> str:
        """ê¸°ì¡´ infer ë©”ì„œë“œ í˜¸í™˜ì„± ìœ ì§€"""
        try:
            # ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰
            if inference_type == InferenceType.IMPACT:
                return self._infer_impact_sync(release_name, series_info)
            elif inference_type == InferenceType.LEVEL:
                return self._infer_level_sync(release_name, series_info)
            elif inference_type == InferenceType.DESCRIPTION_KO:
                return self._infer_description_ko_sync(release_name, series_info)
            else:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶”ë¡  íƒ€ì…: {inference_type}")
                return "MEDIUM"  # ê¸°ë³¸ê°’
        except Exception as e:
            logger.error(f"ì¶”ë¡  ì‹¤íŒ¨ ({inference_type}): {e}")
            return "MEDIUM"  # ê¸°ë³¸ê°’

    def _infer_impact_sync(self, release_name: str, series_info: Dict[str, Any]) -> str:
        """ì˜í–¥ë„ ì¶”ë¡  (ë™ê¸° ë°©ì‹)"""
        prompt_template = """
        ë‹¤ìŒ ê²½ì œ ì§€í‘œì˜ ì‹œì¥ ì˜í–¥ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.
        
        ì˜ˆì‹œ:
        ì§€í‘œëª…: Consumer Price Index
        ì œëª©: Consumer Price Index for All Urban Consumers: All Items
        ì„¤ëª…: Measures the average change over time in the prices paid by urban consumers for a market basket of consumer goods and services.
        ì¶œì²˜: U.S. Bureau of Labor Statistics
        ì‘ë‹µ: HIGH
        
        ì§€í‘œëª…: Industrial Production
        ì œëª©: Industrial Production: Manufacturing
        ì„¤ëª…: Measures the real output of all relevant establishments located in the United States.
        ì¶œì²˜: Board of Governors of the Federal Reserve System
        ì‘ë‹µ: MEDIUM
        
        ì§€í‘œëª…: Housing Starts
        ì œëª©: Housing Starts: Total: New Privately Owned Housing Units Started
        ì„¤ëª…: The number of new residential construction projects that have begun during any particular month.
        ì¶œì²˜: U.S. Census Bureau
        ì‘ë‹µ: MEDIUM
        
        ì§€í‘œëª…: {name}
        ì œëª©: {title}
        ì„¤ëª…: {notes}
        ì¶œì²˜: {source}
        
        ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”: HIGH, MEDIUM, LOW
        """
        
        input_data = {
            "name": release_name,
            "title": series_info.get("title", ""),
            "notes": series_info.get("notes", ""),
            "source": series_info.get("source", "")
        }
        
        # ë™ê¸° ë°©ì‹ìœ¼ë¡œ LLM í˜¸ì¶œ
        chain = self.background_service._create_chain(prompt_template)
        config = self.background_service.langfuse_manager.get_callback_config()

        result = chain.invoke(input_data, config=config)
        raw_response = result.content.strip().upper() if hasattr(result, "content") else str(result).strip().upper()
        
        # String parsingìœ¼ë¡œ ì •í™•í•œ ê°’ ì¶”ì¶œ
        return self._parse_impact_response(raw_response)
    
    def _parse_impact_response(self, response: str) -> str:
        """ì˜í–¥ë„ ì‘ë‹µ íŒŒì‹± ë° ì •ê·œí™”"""
        # ì •í™•í•œ ê°’ì´ ìˆëŠ” ê²½ìš°
        if response in ["HIGH", "MEDIUM", "LOW"]:
            return response
        
        # ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
        if "HIGH" in response:
            return "HIGH"
        elif "MEDIUM" in response:
            return "MEDIUM"
        elif "LOW" in response:
            return "LOW"
        
        # ê¸°ë³¸ê°’ (ê°€ì¥ ì•ˆì „í•œ ì˜µì…˜): "MEDIUM"ì€ ê· í˜•ì¡íŒ ì˜í–¥ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        # ê³¼ëŒ€í‰ê°€(HIGH)ë‚˜ ê³¼ì†Œí‰ê°€(LOW)ì˜ ìœ„í—˜ì„ ìµœì†Œí™”í•˜ì—¬ ì˜ëª»ëœ ìš°ì„ ìˆœìœ„ë‚˜
        # ë¦¬ì†ŒìŠ¤ í• ë‹¹ìœ¼ë¡œ ì¸í•œ ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
        return "MEDIUM"

    def _infer_level_sync(self, release_name: str, series_info: Dict[str, Any]) -> str:
        """ë ˆë²¨ ì¶”ë¡  (JSON ì‘ë‹µ, ë™ê¸° ë°©ì‹)"""
        prompt_template = """
        ë‹¤ìŒ ê²½ì œ ì§€í‘œì˜ ì‚¬ìš©ì ë ˆë²¨ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:
        
        ì§€í‘œëª…: {name}
        ì œëª©: {title}
        ì„¤ëª…: {notes}
        ì¶œì²˜: {source}
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "level": "BEGINNER|INTERMEDIATE|ADVANCED",
            "level_category": "ë¶„ë¥˜ëª…"
        }}
        """
        
        input_data = {
            "name": release_name,
            "title": series_info.get("title", ""),
            "notes": series_info.get("notes", ""),
            "source": series_info.get("source", "")
        }
        
        # ë™ê¸° ë°©ì‹ìœ¼ë¡œ LLM í˜¸ì¶œ
        chain = self.background_service._create_chain(prompt_template)
        config = self.background_service.langfuse_manager.get_callback_config()

        result = chain.invoke(input_data, config=config)
        return result.content if hasattr(result, "content") else str(result)

    def _infer_description_ko_sync(self, release_name: str, series_info: Dict[str, Any]) -> str:
        """í•œê¸€ ì„¤ëª… ì¶”ë¡  (ë™ê¸° ë°©ì‹)"""
        prompt_template = """
        ë‹¤ìŒ ê²½ì œ ì§€í‘œì— ëŒ€í•œ ê°„ë‹¨í•œ í•œê¸€ ì„¤ëª…ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        
        ì§€í‘œëª…: {name}
        ì œëª©: {title}
        ì„¤ëª…: {notes}
        ì¶œì²˜: {source}
        
        ì¼ë°˜ì¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ì„¤ëª…ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        input_data = {
            "name": release_name,
            "title": series_info.get("title", ""),
            "notes": series_info.get("notes", ""),
            "source": series_info.get("source", "")
        }
        
        # ë™ê¸° ë°©ì‹ìœ¼ë¡œ LLM í˜¸ì¶œ
        chain = self.background_service._create_chain(prompt_template)
        config = self.background_service.langfuse_manager.get_callback_config()

        result = chain.invoke(input_data, config=config)
        return result.content.strip() if hasattr(result, "content") else str(result).strip()

    # ê¸°ì¡´ async ë©”ì„œë“œë“¤ (í˜¸í™˜ì„± ìœ ì§€)
    def _infer_impact(self, release_name: str, series_info: Dict[str, Any]) -> str:
        """ì˜í–¥ë„ ì¶”ë¡  (async ë°©ì‹)"""
        prompt_template = """
        ë‹¤ìŒ ê²½ì œ ì§€í‘œì˜ ì‹œì¥ ì˜í–¥ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.
        
        ì˜ˆì‹œ:
        ì§€í‘œëª…: Consumer Price Index
        ì œëª©: Consumer Price Index for All Urban Consumers: All Items
        ì„¤ëª…: Measures the average change over time in the prices paid by urban consumers for a market basket of consumer goods and services.
        ì¶œì²˜: U.S. Bureau of Labor Statistics
        ì‘ë‹µ: HIGH
        
        ì§€í‘œëª…: Industrial Production
        ì œëª©: Industrial Production: Manufacturing
        ì„¤ëª…: Measures the real output of all relevant establishments located in the United States.
        ì¶œì²˜: Board of Governors of the Federal Reserve System
        ì‘ë‹µ: MEDIUM
        
        ì§€í‘œëª…: Housing Starts
        ì œëª©: Housing Starts: Total: New Privately Owned Housing Units Started
        ì„¤ëª…: The number of new residential construction projects that have begun during any particular month.
        ì¶œì²˜: U.S. Census Bureau
        ì‘ë‹µ: MEDIUM
        
        ì§€í‘œëª…: {name}
        ì œëª©: {title}
        ì„¤ëª…: {notes}
        ì¶œì²˜: {source}
        
        ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”: HIGH, MEDIUM, LOW
        """
        
        input_data = {
            "name": release_name,
            "title": series_info.get("title", ""),
            "notes": series_info.get("notes", ""),
            "source": series_info.get("source", "")
        }
        
        result = asyncio.run(self.background_service.process_with_llm(prompt_template, input_data))
        raw_response = result.strip().upper()
        
        # String parsingìœ¼ë¡œ ì •í™•í•œ ê°’ ì¶”ì¶œ
        return self._parse_impact_response(raw_response)

    def _infer_level(self, release_name: str, series_info: Dict[str, Any]) -> str:
        """ë ˆë²¨ ì¶”ë¡  (JSON ì‘ë‹µ, async ë°©ì‹)"""
        prompt_template = """
        ë‹¤ìŒ ê²½ì œ ì§€í‘œì˜ ì‚¬ìš©ì ë ˆë²¨ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:
        
        ì§€í‘œëª…: {name}
        ì œëª©: {title}
        ì„¤ëª…: {notes}
        ì¶œì²˜: {source}
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "level": "BEGINNER|INTERMEDIATE|ADVANCED",
            "level_category": "ë¶„ë¥˜ëª…"
        }}
        """
        
        input_data = {
            "name": release_name,
            "title": series_info.get("title", ""),
            "notes": series_info.get("notes", ""),
            "source": series_info.get("source", "")
        }
        
        result = asyncio.run(self.background_service.process_with_llm(prompt_template, input_data))
        return result

    def _infer_description_ko(self, release_name: str, series_info: Dict[str, Any]) -> str:
        """í•œê¸€ ì„¤ëª… ì¶”ë¡  (async ë°©ì‹)"""
        prompt_template = """
        ë‹¤ìŒ ê²½ì œ ì§€í‘œì— ëŒ€í•œ ê°„ë‹¨í•œ í•œê¸€ ì„¤ëª…ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        
        ì§€í‘œëª…: {name}
        ì œëª©: {title}
        ì„¤ëª…: {notes}
        ì¶œì²˜: {source}
        
        ì¼ë°˜ì¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ì„¤ëª…ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        input_data = {
            "name": release_name,
            "title": series_info.get("title", ""),
            "notes": series_info.get("notes", ""),
            "source": series_info.get("source", "")
        }
        
        result = asyncio.run(self.background_service.process_with_llm(prompt_template, input_data))
        return result.strip()


# ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ LLMServiceFactory í´ë˜ìŠ¤
class LLMServiceFactory:
    """LLM ì„œë¹„ìŠ¤ íŒ©í† ë¦¬ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„± ìœ ì§€)"""

    @staticmethod
    def create_service() -> LLMInferenceService:
        """LLMInferenceService ìƒì„± (ì–´ëŒ‘í„° ì‚¬ìš©)"""
        background_service = BackgroundLLMService()
        return LLMInferenceService(background_service) 
