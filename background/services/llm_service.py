"""
LLM ì„œë¹„ìŠ¤ - SOLID ì›ì¹™ì„ ì¤€ìˆ˜í•œ LLM ì¶”ë¡  ë¡œì§
"""
import os
import time
import logging
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from enum import Enum

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# core ëª¨ë“ˆì˜ ê³µí†µ LLM ë¡œì§ ì‚¬ìš©
from app.core.llm import LLMFactory, BaseLLMProvider, LangfuseManager
from app.constants import UserLevel, ImpactLevel
from ..prompts.etl_prompts import impact_prompt, level_prompt, description_ko_prompt

logger = logging.getLogger(__name__)


class InferenceType(str, Enum):
    """ì¶”ë¡  íƒ€ì…"""
    IMPACT = "impact"
    LEVEL = "level"
    DESCRIPTION_KO = "description_ko"


class BaseInferenceStrategy(ABC):
    """ì¶”ë¡  ì „ëµ ì¸í„°í˜ì´ìŠ¤ (Strategy Pattern)"""

    @abstractmethod
    def get_prompt_template(self) -> str:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë°˜í™˜"""
        pass

    @abstractmethod
    def get_valid_values(self) -> list:
        """ìœ íš¨í•œ ê°’ë“¤ ë°˜í™˜"""
        pass

    @abstractmethod
    def get_default_value(self) -> str:
        """ê¸°ë³¸ê°’ ë°˜í™˜"""
        pass

    @abstractmethod
    def process_result(self, result: str) -> str:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        pass


class ImpactInferenceStrategy(BaseInferenceStrategy):
    """ì˜í–¥ë„ ì¶”ë¡  ì „ëµ"""

    def get_prompt_template(self) -> str:
        return impact_prompt

    def get_valid_values(self) -> list:
        return [level.value for level in ImpactLevel]

    def get_default_value(self) -> str:
        return ImpactLevel.MEDIUM.value

    def process_result(self, result: str) -> str:
        result = result.strip().upper()
        if result in self.get_valid_values():
            return result
        return self.get_default_value()


class LevelInferenceStrategy(BaseInferenceStrategy):
    """ë ˆë²¨ ì¶”ë¡  ì „ëµ"""

    def get_prompt_template(self) -> str:
        return level_prompt

    def get_valid_values(self) -> list:
        return [level.value for level in UserLevel]

    def get_default_value(self) -> str:
        return UserLevel.ADVANCED.value

    def process_result(self, result: str) -> str:
        # NOTE: level_promptê°€ JSONì„ ë°˜í™˜í•˜ë¯€ë¡œ, í›„ì²˜ë¦¬ëŠ” ì—¬ê¸°ì„œ í•˜ì§€ ì•Šê³ 
        # ì´ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ëŠ” ìª½(EventMapper)ì—ì„œ ì§ì ‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        # ë”°ë¼ì„œ ì—¬ê¸°ì„œëŠ” ë°›ì€ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        return result


class DescriptionKoInferenceStrategy(BaseInferenceStrategy):
    """í•œê¸€ ì„¤ëª… ì¶”ë¡  ì „ëµ"""

    def get_prompt_template(self) -> str:
        return description_ko_prompt

    def get_valid_values(self) -> list:
        return []  # ììœ  í˜•ì‹ í…ìŠ¤íŠ¸ì´ë¯€ë¡œ ìœ íš¨ì„± ê²€ì‚¬ ì—†ìŒ

    def get_default_value(self) -> str:
        return "ê²½ì œ ì§€í‘œ ì •ë³´"  # ê¸°ë³¸ ì„¤ëª…

    def process_result(self, result: str) -> str:
        result = result.strip()
        if len(result) > 0:
            return result
        return self.get_default_value()


class RetryConfig:
    """ì¬ì‹œë„ ì„¤ì • í´ë˜ìŠ¤ (Single Responsibility)"""

    def __init__(self, max_retries: int = 3, base_delay: float = 0.5):
        self.max_retries = max_retries
        self.base_delay = base_delay


class LLMInferenceService:
    """LLM ì¶”ë¡  ì„œë¹„ìŠ¤ (Single Responsibility + Dependency Injection)"""

    def __init__(
        self,
        provider: BaseLLMProvider,
        retry_config: Optional[RetryConfig] = None
    ):
        self.provider = provider
        self.retry_config = retry_config or RetryConfig()
        self._strategies = {
            InferenceType.IMPACT: ImpactInferenceStrategy(),
            InferenceType.LEVEL: LevelInferenceStrategy(),
            InferenceType.DESCRIPTION_KO: DescriptionKoInferenceStrategy(),
        }
        # Langfuse Manager ì´ˆê¸°í™”
        self.langfuse_manager = LangfuseManager(service_name="background_etl")

    def infer(
        self,
        inference_type: InferenceType,
        release_name: str,
        series_info: Dict[str, Any]
    ) -> str:
        """ì•ˆì „í•œ ì¶”ë¡  ì‹¤í–‰ (DRY ì›ì¹™ ì¤€ìˆ˜)"""
        strategy = self._strategies[inference_type]

        for attempt in range(self.retry_config.max_retries):
            try:
                return self._execute_inference(strategy, release_name, series_info)
            except Exception as e:
                logger.warning(
                    f"[{inference_type.value} ì¶”ë¡ ] LLM í˜¸ì¶œ ì‹¤íŒ¨ "
                    f"(ì‹œë„ {attempt + 1}/{self.retry_config.max_retries}): {e}"
                )

                if attempt < self.retry_config.max_retries - 1:
                    wait_time = self.retry_config.base_delay * (2 ** attempt)
                    logger.info(f"[{inference_type.value} ì¶”ë¡ ] {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"[{inference_type.value} ì¶”ë¡ ] ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜")
                    return strategy.get_default_value()

    def _execute_inference(
        self,
        strategy: BaseInferenceStrategy,
        release_name: str,
        series_info: Dict[str, Any]
    ) -> str:
        """ì‹¤ì œ ì¶”ë¡  ì‹¤í–‰ (Private ë©”ì„œë“œë¡œ ìº¡ìŠí™”)"""
        llm = self.provider.create_llm()

        prompt_template = ChatPromptTemplate.from_template(strategy.get_prompt_template())
        chain = prompt_template | llm | StrOutputParser()

        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        input_data = {
            "title": series_info.get("title", ""),
            "name": release_name,
            "notes": series_info.get("notes", ""),
            "source": series_info.get("source", "")
        }

        # Langfuse callback ì„¤ì • (ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)
        config = self.langfuse_manager.get_callback_config()

        logger.info(f"ğŸš€ LLM ì˜ˆì¸¡ ì‹œì‘: title={input_data['title']}, name={input_data['name']}")
        logger.info(f"ğŸ“ Config: {config}")

        # LLM í˜¸ì¶œ ë° Langfuse ì¶”ì 
        try:
            result = chain.invoke(input_data, config=config)
            logger.info(f"âœ… LLM í˜¸ì¶œ ì„±ê³µ: raw_result type={type(result)}")
        except Exception as e:
            logger.error(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise

        processed_result = strategy.process_result(result)

        logger.info(f"ğŸ¯ LLM ì˜ˆì¸¡ ê²°ê³¼: {processed_result}")

        # Background ì‘ì—…ì´ë¯€ë¡œ ìˆ˜ë™ìœ¼ë¡œ flush (ì¤‘ìš”!)
        self.langfuse_manager.flush_events()

        logger.info(f"ğŸ“Š Langfuse ì¶”ì  ì™„ë£Œ - ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸ ê°€ëŠ¥")

        return processed_result


class LLMServiceFactory:
    """LLM ì„œë¹„ìŠ¤ íŒ©í† ë¦¬ (Factory Pattern + Configuration)"""

    @classmethod
    def create_service(cls) -> LLMInferenceService:
        """ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ LLM ì„œë¹„ìŠ¤ ìƒì„±"""
        # core ëª¨ë“ˆì˜ Factory ì‚¬ìš©, temperature=0ìœ¼ë¡œ ETLìš© ì„¤ì •
        provider = LLMFactory.create_provider(
            provider_type=None,  # ê¸°ë³¸ê°’ ì‚¬ìš© (settingsì—ì„œ ì½ìŒ)
            model=None,          # ê¸°ë³¸ê°’ ì‚¬ìš© (settingsì—ì„œ ì½ìŒ)
            temperature=0        # ETLìš©ìœ¼ë¡œ deterministicí•˜ê²Œ
        )

        retry_config = RetryConfig(
            max_retries=int(os.getenv('LLM_MAX_RETRIES', '3')),
            base_delay=float(os.getenv('LLM_API_DELAY', '0.5'))
        )

        return LLMInferenceService(provider, retry_config) 
